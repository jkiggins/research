\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{subcaption}
\usepackage{float}
\usepackage{booktabs, multirow} % for borders and merged ranges



\usepackage[backend=biber,bibencoding=utf8,style=ieee]{biblatex}
%% \usepackage[backend=biber, bibencoding=utf8]{biblatex}
\addbibresource{mybibliography.bib}

% Typset indexes - Needed for sorting the glossary
%  - xindy: Sorting / indexing of items
\usepackage[xindy]{imakeidx}

% Support for glossaries
%  - nopostdot: Omit dot at the end of each description
%  - nonumberlist: Supress number of items
%  - acronym: Support for acronyms
%  - toc: Add glossary to table of contents
%  - xindy: Sorting / indexing of items
\usepackage[nopostdot,nonumberlist,acronym,toc,xindy]{glossaries}

% Support for pretty inline fractions
\usepackage{nicefrac}

% SVG package to include SVGs
\usepackage{svg}

\usepackage[export]{adjustbox}

% Aliases for common names
\newcommand{\ca}{\gls{ca}\textrm{ }}
\newcommand{\cam}{Ca^{2+}}
\newcommand{\na}{$\textrm{Na}^+$\textrm{ }}
\newcommand{\ipt}{\gls{ipt}\textrm{ }}
\newcommand{\kp}{\gls{kp}\textrm{ }}
\newcommand{\dser}{\gls{dser}\textrm{ }}
\newcommand{\serca}{\gls{serca}\textrm{ }}

% Usage: \afig{url}{Figure caption}{label for referencing later}
\newcommand{\afig}[3]{
	\begin{figure}[H]
    	\centering
		\includegraphics[width=\linewidth]{#1}
        \caption{#2.}
        \label{#3}
	\end{figure}
}

% Usage: \afigf{url}{Figure caption}{label for referencing later}
\newcommand{\afigf}[3]{
	\begin{figure}[h]
    	\centering
		\includegraphics[width=\linewidth]{#1}
        \caption{#2.}
        \label{#3}
	\end{figure}
}

\newcommand{\asvgf}[4]{
	\begin{figure}[!htbp]
    	\centering
		\adjustbox{max width=#4\linewidth}{\includesvg{#1}}
        \caption{#2.}
        \label{#3}
	\end{figure}
}

% Usage: \afigw{url}{Figure caption}{label for referencing later}{width 1/factor}
\newcommand{\afigw}[4]{
	\begin{figure}[H]
    	\centering
		\includegraphics[width=\linewidth/#4]{#1}
        \caption{#2.}
        \label{#3}
	\end{figure}
}

% Usage: \afigs{url}{Figure caption}{label for referencing later}{width}
\newcommand{\afigs}[4]{
	\begin{figure}[H]
    	\centering
		\includegraphics[width=#4\columnwidth]{#1}
        \caption{#2.}
        \label{#3}
	\end{figure}
}

% Equation referencing
\newcommand{\eq}[1]{Equation (\ref{#1})}

% Algorithm referencing
\newcommand{\alg}[1]{Algorithm~\ref{#1}}

% Change comment style to use #
%% \algrenewcommand{\algorithmiccomment}[1]{\# #1}

% make *proper* vector arrows - Credit to harpoon pacakge for initial idea
\newlength{\argwd}
\newlength{\arght}
\newcommand{\overharp}[3]{%
	\settowidth{\argwd}{#2}%
	\settoheight{\arght}{#2}%
	\addtolength{\argwd}{.1\argwd}%
	\raisebox{\arght}{%
		\makebox[.04\argwd][l]{%
			\resizebox{\argwd}{#3\arght}{$#1$}%
		}%
	}%
	#2%
}
\newcommand{\overrightharp}[2]{\overharp{\rightharpoonup}{#1}{#2}}
\newcommand{\vect}[2][.5]{\text{\overrightharp{\ensuremath{\boldsymbol{#2}}}{#1}}}
\newcommand{\vectmd}[2][.5]{\text{\overrightharp{\ensuremath{#2}}{#1}}}

% Make *proper* text over sim - Credit: http://tex.stackexchange.com/a/43338/66603
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand{\distas}[1]{%
  \savebox{\mybox}{\hbox{$\scriptstyle#1$}}%
  \savebox{\mysim}{\hbox{$\sim$}}%
  \mathbin{\overset{#1}{\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}

% Load the acronyms
\loadglsentries[type=\acronymtype]{acronym}

% Initialize the glossary
\makeglossaries
\setglossarystyle{index}

% Sort the glossary
\makeindex


%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{A Bio-Inspired Computational Astrocyte Model for Spiking Neural Networks}

\author{\IEEEauthorblockN{Anonymous Authors}}

%% \author{\IEEEauthorblockN{Jacob Kiggins}
%% \IEEEauthorblockA{\textit{Computer Engineering Dept.} \\
%% \textit{Rochester Institute of Technology}\\
%% Rochester, NY \\
%% jmk1154@rit.edu}
%% \and
%% \IEEEauthorblockN{Dr. J. David Schaffer}
%% \IEEEauthorblockA{\textit{College of Community and Public Affairs} \\
%% \textit{Binghamton University}\\
%% Binghamton, NY \\
%% dschaffe@binghamton.edu}
%% \and
%% \IEEEauthorblockN{Dr. Cory Merkel}
%% \IEEEauthorblockA{\textit{Computer Engineering Dept.} \\
%% \textit{Rochester Institute of Technology}\\
%% Rochester, NY \\
%% cemeec@rit.edu}}


\begin{document}

\maketitle

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The mammalian brain is the most capable and complex computing entity known
today, and for years there has been research focused on reproducing the brain's
capabilities. An early example of this endeavor was the perceptron which has
become a core building block of neural network models in the deep learning
era. Despite many successes achieved through deep learning, these networks
behave much differently than their biological counterparts. In a search for
improvements to things like training time and dataset size, power consumption,
noise, and adversarial input tolerance, research is looking towards the brain,
and bio-inspired computing models. \Glspl{snn} take a step closer to biology in
their operation and are the focus of much research, geared towards reproducing
some of these novel features of the brain. As of yet, \glspl{snn} have not
reached their full potential. This work explores the advancement of \glspl{snn}
though introduction of a novel astrocyte model. Astrocytes, initially thought to
be passive support cells in the brain are now known to actively participate in
neural processing. The proposed astrocyte model is geared towards synaptic
plasticity and is shown to generalize and extend \gls{stdp}. In addition, the
model supports plasticity-focused multi-synapse integration. Logical AND was
used as a test function for multi-synapse astrocyte plasticity, and convergence
for a single \gls{lif} neuron with 2, 3, and 4 synapses was demonstrated. The
plasticity model is general and many other functions could presumably be used
in-place of AND, taking advantage of a multi-synapse view. This leaves a solid
path forward for future work.

\end{abstract}

\begin{IEEEkeywords}
Spiking Neural Networks, Astrocytes, Neuromorphic
\end{IEEEkeywords}

\section{Introduction}

%% The human brain exhibits may features that current approaches to machine
%% learning and artificial intelligence are unable to mimic. Certainly, deep
%% learning has achieved fantastic results in a variety of specialized tasks, but
%% this success comes with a cost. Increasingly higher parameters counts are needed
%% to achieve improvements, and with each parameter comes additional time and
%% energy for training. Once trained, computing a deep network's decision in
%% real-time requires specialized, power-hungry hardware.

%%  Where a deep learning model
%% requires many diverse examples to approach generalization, the human brain is
%% able to surpass this with only a few examples \parencite{tsimenidis_2020}. Even
%% more impressive, the brain can accomplish these feats with $\approx$ 20 Watts of
%% power.

The human brain exhibits very high levels of connectivity, is able to handle a
wide variety of complex tasks, and has many advantages in the context of
learning, and tolerance to adversarial inputs. Where a deep learning model
requires many diverse examples to approach generalization, the human brain is
able to surpass this with only a few examples \parencite{tsimenidis_2020}. Even
more impressive, the brain can accomplish these feats with $\approx 20$ Watts of
power. Research in the intersection between neuroscience and computation
attempts to understand the mechanisms used by the brain to achieve these
feats. Neuroscientists are continuously making new discoveries, and computer
scientists are taking inspiration from these discoveries in their modeling
efforts. One such discovery implicated astrocytes as active participants in
neural processing, where before they were considered support cells. This
discovery lead to the development of a wide variety of astrocyte models
\cite{manninen_2019}. Many of these were highly grounded in biochemistry, and
focused on reproducing observed astrocyte behavior. Given the clear role for
astrocytes in computation and learning \parencite{mederos_2018}, this work takes
a slightly different approach, and looks to leverage astrocytes to enhance the
processing capabilities of \glspl{snn}. In general \glspl{snn} are difficult to
train, especially at a network level, and this is holding back \gls{snn} despite
their rich dynamics and advantages with time-series data
\parencite{tavanaei_2019}. This work attempts to close these gaps further,
through the development and exploration of a computational astrocyte model,
inspired by biology and previous modeling efforts. The main contributions are

\begin{itemize}
  \item A computational astrocyte model, with a design geared towards synaptic
    plasticity.
  \item Coupled with a \gls{lif} neuron, the astrocyte model was shown to
    implement a general form of classic \gls{stdp}, and improve local learning
    through temporal integration.
  \item Multi-synapse astrocyte plasticity was defined and explored with
    success, using logical AND as a test function.
\end{itemize}

%% There are a handful of computational astrocyte models,
%% but in general they fail to clearly define a functional role. Works that do,
%% don't focus on aspects that solve key problems in \glspl{snn}
%% \parencite{manninen_2019}. This leaves a clear need for a computational
%% astrocyte model that focuses on extracting key features from biology, and
%% advances the functionality of \glspl{snn}.

\section{Background and Related Work}
\subsection{Spiking Neural Networks}
\Glspl{snn} are a type of brain-inspired computing model and the most
bio-realistic used in engineering applications. \Glspl{sn} emit spikes,
reminiscent of a biological neuron's action potentials. These spikes travel
across a synapse, from one neuron to the post synaptic terminal of
another. During transmission this spike is usually multiplied by a weight value,
but depending on the model things like diffusion and probability of release may
come into play. At the downstream neuron's post-synaptic terminal, spikes from
one or more synapses arrive and are transformed into a voltage wave, called a
\gls{psp}. \glspl{psp} are added over space and time, and accumulate at the
neuron downstream in the form of membrane voltage. An activation function of
this voltage determines if and when a spiking neuron fires. Generally,
once a certain threshold is reached the post-synaptic neuron emits a voltage
spike (fires).

There are many potential models that fit the requirements of a spiking
neuron. Figure \ref{fig:sn_model_compare} shows where a number of models fall
when measured against their bio-realism \parencite{hendy_2022}. Specifically,
the X-axis shows the number of floating point operations per-second (FLOPS) are
required to simulate the model in real-time. In the Y-axis counts the number of
cortical ``features'' out of a total of $22$ that model is able to reproduce.

\afigf{figures/JEI_31_1_010901_f004.png}{Comparison of spiking neuron
  models}{fig:sn_model_compare}

\subsubsection{Leaky Integrate and Fire Neurons}
Due to its simplicity and computational efficiency the most popular \gls{sn}
model explored in \gls{snn} literature is the \gls{lif} neuron. In this model
there is an internal voltage, which increases as spikes arrive at the
post-synaptic terminal of the neuron. Charge leaks from this internal reservoir
over time, lowering the voltage \parencite{ponulak_2011}.

%% \eq{eq:lif} defines the behavior of a generic
%% \gls{lif} model \parencite{izhikevich_2004}.

%% \begin{align}
%%   \frac{du}{dt}(t) = -\tau_{mem}u(t)+(i_o + \Sigma w_jz_j(t)) \label{eq:lif}
%% \end{align}

%% In \eq{eq:lif}, $\tau$ is the leaking time-constant, with $i_o$
%% representing a constant leak, independent of $u$. $\Sigma w_ji_j(t)$ is the
%% increase in $u$ at time $t$, given the presence of a spike $z_j$ on synapse
%% $j$, and the synaptic weight $w_j$. Once the state variable u (generally
%% thought of as a voltage) reaches a certain threshold the neuron outputs a
%% spike. After this time the state variable is reset, and is held at that
%% value for a period known as the absolute refractory period. This mimics the
%% high level behavior of biological neurons \parencite{ponulak_2011}.

%% \Gls{lif}
%% neurons are fairly simple computationally, relying on exponential functions
%% and thresholds in the functional definition.

This work makes use of a \gls{lif} neuron model which like other
integrate-and-fire type models is not the most bio-realistic
\parencite{izhikevich_2004}. However, \gls{lif} neurons exhibit rich behavior
with few parameters, and are easily extended. The goal here, is to remain
computationally simple, while still leveraging unique features found in biology.

Equations (\ref{eq:lif:psp}), and (\ref{eq:lif:v}) define the differential equations that
govern neuron \gls{psp} $v_{syn}$ and membrane voltage $v_{mem}$
dynamics.

\begin{align}
d_{vsyn} = -v_{syn}(t) \tau_{syn} d_t + z_{pre}(t) w \label{eq:lif:psp} \\
d_{vmem} = -\tau_{mem} (v_{mem} + v_{syn}) d_t \label{eq:lif:v}
\end{align}

Here, $z_{pre}$ represents the spiking activity on the synapse at time $t$ and
has a value of either $0$ or $1$. $w$ is the synaptic weight. $\tau_{syn}$ and
$\tau_{mem}$ are post-synaptic terminal and membrane timing constants. In these
equations $d_t$ represents an small step in time, and $d_{vsyn/vmem}$
corresponding steps in voltage. During simulation for this work the time-step
was fixed to 1ms.

The \gls{lif} neuron fires according to \eq{eq:lif:fire}, where $H$ is the
Heaviside step function, and $thr_{mem}$ is the $v_{mem}$ threshold that
triggers firing.

\begin{align}
z_{post}(t) = H(v_{mem}(t) - thr_{mem}) \label{eq:lif:fire}
\end{align}

After firing the neuron membrane potential is reset to $v_{reset}$, which is
$\leq -thr_{mem}$ throughout this work, see \eq{eq:lif:reset}.

\begin{align}
v_{mem}(t+1) = v_{mem}(t)(1 - z_{post}(t)) + z_{post}(t)v_{reset} \label{eq:lif:reset}
\end{align}

%% Figure \ref{fig:1n1s1a_fn_diagram} depicts the \gls{lif} neuron described by
%% equations above and Figure \ref{fig:lif:sample_1} shows a sample of the
%% \gls{lif} response to a random Poisson input.
%% values. Table \ref{table:lif_params}
%% defines the baseline parameter values used throughout the work, with any
%% differences being outlined in figures.

%% \begin{table}[!htp]\centering
%%   \caption{\Gls{lif} neuron baseline parameters} \label{table:lif_params}
%%   \scriptsize
%%   \begin{tabular}{c|c|c|c|}
%%     Reset Voltage & Membrane Threshold & Membrane $\tau$ & Synaptic $\tau$ \\
%%     -0.2 v & 0.2 v & 60 1\/s & 500 1\/s \\
%%   \end{tabular}
%% \end{table}

%% \asvgf{figures/artifacts/obj1/lif_sample_mem-60.0_syn-500.0.svg}{\Gls{lif} Neuron
%% response to poisson input. $\tau_{mem}=60$ and
%% $\tau_{syn}=500$}{fig:lif:sample_1}{0.75}

\subsection{Spiking Neural Network Learning Approaches}
In biology, the changing of synaptic efficacy is referred to as synaptic
plasticity, and is considered to be one method that facilitates learning and
memory. Changes in plasticity can be transient, such as with pulse-paired
facilitation or more permanent, such as with long-term potentiation
\parencite{ponulak_2011}.


%%%%%%%%%%%% \gls{stdp} %%%%%%%%%%%%
\subsubsection{Spike-Timing Dependent Plasticity}
One of the most widely used learning approaches employed in \glspl{snn} is
\gls{stdp}, which is an unsupervised approach and variation of Hebbian
learning. Classic Hebbian learning can be summed up by ``if neurons fire
together, they wire together'' meaning coincident firing is rewarded by
increased synaptic strength. Rate-based Hebbian learning strengthens a
synapse in response to correlated firing, as shown by Figure
\ref{fig:heb_rate_stdp}. \Gls{stdp} (also depicted in that figure) is a
spike-timing based variation, and looks for causation. This means a
connection is strengthened if a pre-synaptic, input spike is followed by
post-synaptic spike (downstream neuron fires), and weakened if a
pre-synaptic spike follows post-synaptic spike.

\asvgf{figures/heb_diagrams.svg}{Diagram showing \gls{stdp} and rate-based
  hebbian learning $\Delta w$ response}{fig:heb_rate_stdp}{0.7}

\eq{eq:classic_stdp} describes classic \gls{stdp}
\parencite{tavanaei_2019}. $\tau$ in this case is a decay constant, which
sets up en effective time window for the learning rule. If spikes occur
further apart than this window, they have little or no effect on synaptic
weight.

\begin{align}
  \Delta W =
  \begin{cases}
    Ae^{-\frac{|t_{pre}-t_{post}|}{\tau}} & t_{pre} - t_{post} \leq 0, A > 0
    \\ Be^{\frac{|t_{pre}-t_{post}|}{\tau}} & t_{pre} - t_{post} > 0, B < 0
  \end{cases} \label{eq:classic_stdp}
\end{align}

Swapping the conditions in \eq{eq:classic_stdp} leads to
\gls{astdp} where correlated spikes result in reduced weight. This
is useful in some learning rules extending \gls{stdp}.

Classic unsupervised \gls{stdp} has seen some success in pattern recognition
tasks, but does fall short in other areas. In general \gls{stdp} converges to
recognize repeating spatio-temporal patterns, and complex patterns can be
recognized by a single neuron \parencite{tavanaei_2019}. \Gls{stdp} does have
trouble converging in order to differentiate similar (but distinct) inputs
\parencite{vigneron_2020}. \Gls{stdp} also tends to drive weights to favor early
spikes, using those pre-synaptic spikes that arrive first to a neuron as the
main factor in pattern recognition. There has been extensive statistical
analysis of \gls{stdp}. In addition, it has been shown that classic \gls{stdp}
will generally drive weights to either maximally inhibitory, or maximally
excitatory \parencite{legenstein_2005}. Researches found that on average, a
supervised variant of \gls{stdp} (with teaching signal to force output firing)
can converge to a desired min/max weight distribution
\parencite{legenstein_2005}.

\subsubsection{Promising Results With Coordinated STDP}
Much of the success in training \gls{snn} with \gls{stdp} relates back to some
level of coordination, or supervision. For some learning rules there is a reward
signal, for others there is inhibition based on the activity of other neurons,
or some network-level activity.

%% \asvgf{figures/coordinated_stdp.svg}{\gls{lif} neuron with \gls{stdp}
%%   learning and generic coordination signal}{fig:coordinated_stdp}{0.5}

There has been some success and increasing interest with a supervised variant on
\gls{stdp}, known as \gls{rstdp}. Using a deep convolutional approach, a
\glspl{snn} was employed to solve the \gls{mnist} digit recognition task
\parencite{mozafari_2018}. Early layers of the network were trained in an
unsupervised manner, with classic \gls{stdp}. Later layers were updated using
\gls{rstdp}. That is, if the output was correct, \gls{stdp} was applied. If not,
\gls{astdp} was applied. This learning approach has basis in biology, with the
reward modulation mimicking the activity of dopamine \parencite{mozafari_2018}. As
a proof of concept, researchers applied their learning method to a shallow
network, with a single trainable layer. This architecture gave passable results,
but wasn't well suited to multi-layer training. For their deep architecture
which mixed supervised and un-supervised \gls{stdp}, and accuracy of 97.2\% was
achieved \parencite{mozafari_2018}.

%% \afig{figures/rstdp_dcnn_arch.png}{Deep convolutional \gls{snn}
%%   architecture}{fig:rstdp_dcnn}

%% \Gls{rstdp} improves classification on similar samples, and trains the network to
%% recognize discriminating features, instead of repeating ones.

The ReSuMe learning rule is another variant of \gls{stdp}, which is similar to
the \gls{rstdp} rule \parencite{ponulak_2010}. With ReSuMe, a single neuron is
initialized with random weight values. There is a 2nd similar neuron which
implements the desired behavior. Inputs are provided to both neurons, and at
each time step weights are updated according to the difference between the
teaching neuron, and neuron in training. The weight update can be described by:

\begin{align}
  \Delta w = \Delta W_{STDP}(S_{in}, S_{d}) + \Delta W_{aSTDP}(S_{in}, S_{o}) \label{eq:resume_stdp}
\end{align}

\noindent where $\Delta W_{STDP}$ and $\Delta W_{aSTDP}$ are the weight changes
associated with classic \gls{stdp} behavior, and anti- \gls{stdp} behavior. In
each term, the input spike is considered, but first with $S_{d}$ which is the
desired output coming from the ideal neuron. Second, with the actual output
\parencite{mozafari_2018}. Using this learning rule, it is possible to converge,
from an untrained neuron, to one of similar configuration with a desired
behavior.

%% The use of an additional synaptic input and
%% the specifics of \eq{eq:resume_stdp} creates a unique extension to classic
%% \gls{stdp}. Weights can be updated without one of the down-stream neurons
%% firing, and activity on a third synapse is influencing plasticity. Beyond that,
%% this is yet another learning rule, that has shown success when introducing a
%% third factor to \gls{stdp} \parencite{mozafari_2018}.

There are a variety of methods coupled with \gls{stdp} which attempt to force
synapses to be sensitive to different kinds of inputs, instead of learning
redundant responses to inputs. These generally take the form of inhibition,
either of the learning rule, or neuron activity. One specific example, involves
spiking convolution operations, which are trained on image inputs
\parencite{delorme_2001}. Researches employed unsupervised \gls{stdp}, but
forced inhibition between the feature maps. If a neuron fired in one feature
map, the corresponding neuron is inhibited in all others. Results were
promising, and activation of \gls{lif} neuron maps showed selectivity for image
features such as blobs and contours.

\subsection{Introduction to Astrocytes}
Astrocytes are a type of glial cell found in mammalian brains. Their
structure and function are still the topic of cutting edge research
today. They are vital for many normal brain functions, including
cognition, behavior, memory, and learning \parencite{mederos_2018}. In the human
brain astrocytes are known to tightly wrap many synapses, as well as dendrites and cell
bodies. Since astrocytes engulf the synaptic cleft, they are ideally
placed to control extracellular neurotransmitter and ion
concentrations. From a functional point of view, astrocytes listen and
respond to activity at the synaptic clefts of multiple synapses. Activity on
a given synapse leads to a local response, in the astrocyte process that
surrounds that synapse. Local responses from many synapses contribute to a
regional response, and potentially a global, cell-body response. This global
response may propagate to other astrocytes, influencing their activity
\parencite{min_2012}.

At the local level, astrocytes are sensitive to pre and post synaptic
potentials. They sense this activity via uptake of \gls{glu} released at the
presynaptic terminal and \kp ion uptake at the postsynaptic terminal. Sensing
those pre and post-synaptic spikes. The astrocyte will then respond with one or
more \gls{gt}, which have various effects, with some of them listed below:

\begin{itemize}
\item Increase in Probability of Release (PR), or post-synaptic neuron
  sensitivity.
\item Increase in post-synaptic neuron excitability \parencite{chung_2015}.
\item Gating and modulation of \gls{ltp}/\gls{ltd} \parencite{mederos_2018}.
\item Longer-range spatial influence.
\end{itemize}

\Gls{gt} release from an astrocyte is dependent on the integration of synaptic
activity at multiple levels, and across multiple time-scales. At the local level
there is a fast response leading to a release of \gls{gt} at that synapse
\parencite{pitta_2016}. As the responses of many synapses propagate to regional
and global levels there may be coordinated responses at longer timescales, up to
seconds for a global response \parencite{mederos_2018}.

%% The connection between local
%% and cell level response is mediated by \gls{cicr}. As local activity
%% increases beyond a certain threshold, the \ca concentration causes
%% exponential \ca release from the \gls{er}. This \gls{cicr} propagates like a
%% wave, and can reach the main cell body, other astrocyte processes, and even
%% other astrocytes \parencite{manninen_2018}.

\subsection{Functional Roles of Astrocytes}

There are a variety of theories for the functional role of astrocytes.
Some of these are speculation based on general themes of astrocyte behavior,
and others are specific and highly grounded in experimental data.

%% Astrocytes are Master Integrators
Astrocytes have been shown to act as master integrators of synaptic activity at
multiple levels. \ca activity occurs in astrocyte processes local to synapses,
in response to pre and post-synaptic spikes. This activity can (via \ca)
propagate up to regional and global levels, where it is integrated with
signaling from other synapses over a slower timescale. Measured astrocyte \ca
responses are highly complex in time, indicating rich dynamics beyond a simple
on or off \parencite{araque_2014}. There is more evidence of this multi-level
process when looking at the arrangement of surface receptors, and how they form
areas of sensitivity at synapses, or around regions of multiple synapses
\parencite{pitta_2012}.

%% Astrocytes facilitate long-range spatial influence
Astrocytes facilitate long-range spatial communication. A single astrocyte,
through its many end-foot processes can influence many synapses
simultaneously. In the brain, astrocytes are physically distributed via contact
spacing, where their end-foot processes connect at the periphery of an
astrocyte's domain \parencite{pitta_2012}. This spacing is not always uniform,
some micro-domains are formed favoring a neuron signal pathway, with adjacent
astrocyte connections observed to be absent. One example is in the ferret visual
cortex, where astrocytes (like neurons) form receptive fields on the visual
input \parencite{pitta_2012}. This finding lines up well with results in other
works, where astrocytes were used as working-memory units, forming receptive
fields for visual information \parencite{gordleeva_2021}.

%% %% Astrocytes Modulate STP/STD
%% Astrocytes have been shown to modulate short-term synaptic plasticity The
%% pre-synaptic \gls{glu} mediated astrocyte stimulation and response loop, is
%% hypothesized as a mechanism for modulation of short term plasticity
%% \parencite{pitta_2012}. This loop is characterized by positive feedback, where
%% \gls{glu} release from the pre-synaptic neuron stimulates the astrocyte, and
%% leads to additional \gls{glu} release. If the \acrshort{pr} at that synapse is
%% low, then there should be sufficient neural resources to support transmission,
%% and the astrocyte is able to gain-up the response. If \acrshort{pr} is high,
%% then the astrocyte response leads to short-term depression, as neural resources
%% are exhausted quickly.

%% Astrocyte Modulation of \gls{ltd}/\gls{ltp}
Astrocytes have been shown to play a key role in the biochemical pathways that
lead to \gls{stdp}, and plasticity in general at neuronal synapses. They can
gate \gls{ltd} and \gls{ltp} via D-Serine release \parencite{manninen_2019} and
may also reverse the polarity of \gls{stdp}, implementing \gls{astdp}
\parencite{min_2012}. In addition, \Gls{glu} released by astrocytes in response
to pre and post-synaptic activity may stretch or shift the nominal \gls{stdp}
curve in both experimental results, and computational models
\parencite{pitta_2016}. Astrocytes can modulate the concentration of their
surface \gls{glu} transporters in response to activity and in turn increase the
excitability at the post-synaptic terminal for a local region of synapses
\parencite{mederos_2018}.

%% \section{Common Astrocyte Biochemical Pathways}
%% Across both the Neuroscience literature, and existing computational models there
%% exist some common astrocyte signaling pathways. The first of these is the
%% Pre-synatic \gls{glu} pathway. Excitatory cortical neurons release \gls{glu} in
%% response to input stimulus. \Gls{glu} binds to the G-coupled \glspl{mglur} on
%% the surface of an astrocyte. This sets in motion a cascade involving the \ipt
%% second messenger, which ultimately leads to \ca release in the astrocyte cytosol
%% \parencite{pitta_2012}. \ipt induced \ca release from the \gls{er} leads to a
%% rapid breakdown of \ipt, creating a kind of local \ca spike. This \ca
%% concentration can integrate within the cell cytosol, but does degrade due to the
%% activity of pumps at \gls{er} surface \parencite{pitta_2012}.

%% \asvgf{figures/astro_neuron_pathways.svg}{Common astrocyte-neuron pathways,
%%   and common pathways for internal \ca signaling}{fig:astro_pathways}{0.6}

%% Figure \ref{fig:astro_pathways} shows the possible signaling pathways at the
%% tripartite synapse. Each pathway has some underlying research into the
%% behavior at that pathway, either in vitro or in vivo. These experimental data
%% have led to a variety of computational models, which share a common theme but
%% differ in complexity, level of bio-realism, and computational efficiency.

%% Another common pathway that is explored both in neuroscience and
%% computationally is the post-synaptic pathway
%% \parencite{bassam_2015}. When a post-synaptic neuron fires there is \kp release as part of the depolarization process at the
%% postsynaptic terminal. This \kp spillover is quickly shuttled into the
%% astrocyte, and causes depolarization across the astrocyte's
%% membrane. Voltage-gated channels on the \gls{er} then lead to \ca release
%% into the cytoplasm. It is considered fast pathway, because the effect of \kp
%% is direct, vs. the pre-synaptic pathway involving a second messenger
%% \parencite{bassam_2015}.

%% \ca, \ipt and \kp are the main substances involved in signaling pathways
%% within the astrocyte. Looking closely at both the neuroscience literature,
%% and modeling efforts, there are spikes and thresholds within these internal
%% astrocyte concentrations. Neuroscience experiments involving monitoring of
%% astrocyes revealed oscillations, which are more intense and propagate
%% further depending on Activity. These oscillations are the consequence of
%% non-linear negative feedback, leading to fast cleanup, and spike-like
%% behavior \parencite{postnov_2009}. The second messenger \ipt has a similar
%% behavior, with a threshold and $tanh$ (with decay factor) resulting in
%% spike-like behavior \parencite{postnov_2009}. This response was not limited
%% to internal signals, with some models considering spikes of \gls{gt}
%% \parencite{wade_2011}.

%% The amplitude modulated and frequency modulated variants on the $J_{pump} -
%% J_{chan} - J_{leak}$ model exhibit spiking and non-spiking \ca dynamics depending on some choice
%% parameters \parencite{pitta_2009,wade_2011}. It is also well established
%% that the \ipt pathway operates on a slower time-scale, then more direct
%% pathways such as \gls{atp} or \kp \parencite{postnov_2009, bassam_2015}.

%% \section{Evolution of Astrocyte Models}
%% A very insightful review of astrocyte models from 1995 until about 2017 was
%% used to guide the investigation into existing computational astrocyte models
%% \parencite{manninen_2018}. When the existing body of work was considered it
%% was possible to lean on functional similarities between Astrocyte models and
%% identify four distinct core neuron-astrocyte models, which emerged over the
%% time-period considered \parencite{manninen_2018}.

%% Grouping models by their features, it was shown in \parencite{manninen_2018}
%% that only a few fundamental computational astrocyte models make up the core
%% of many published works. These groups were formed by Lin and Rinzel-like
%% models, DeYoung and Keizer, and Hofer.

%% The Hofer model on the other hand, defines an astrocyte model in a more bio-realisitc
%% way, modeling internal chemical signaling using second-order differential
%% equations, and including messengers beyond just \ipt. There is also an
%% emphasis placed on astrocyte to astrocyte communication through gap
%% junctions, which is unique to this model. Overall Hofer-like models were the
%% least prevalent in existing literature \parencite{manninen_2018}.

%% \section{Astrocytes Exhibit Spatial and Temporal Integration}
%% Early research into astrocyte behavior showed cell-wide \ca transients in
%% response to intense activity, which would in some cases propagate to
%% neighboring cells. Around 2010, pharmacological tools became precise enough
%% to discover smaller variations in \ca at astrocyte processes, in response to
%% lower levels of neuronal activity \parencite{manninen_2018}. These smaller
%% variations are thought to be integrated, and lead to a larger cell-level \ca
%% response, though there is not direct evidence of this
%% \parencite{araque_2014}. In addition, the mechanism thought to trigger the
%% switch from local to global responses is \ca-Induced \ca release
%% (\gls{cicr}) which is the mechanism responsible for spike-like behavior
%% within astrocyes.

%% Astrocytes also form their own networks, separate from the connections of
%% neurons. Gap junctions between cells can pass various molecules including
%% ions and secondary messengers. Gap junctions are not evenly distributed, or
%% random, meaning they form meaningful connections between specific
%% astrocytes. These networks appear to form non-overlapping territories, where
%% groups are interconnected, but distinct from other
%% groups \parencite{mederos_2018}. Astrocytes don't have long-reaching
%% projections like the axons of neurons, limiting \ca propagation to a few
%% $\mu m$ \parencite{hofer_2002}. The shapes of these networks are varied,
%% and in the visual cortex consist of between 2 and 10 astrocytes. In
%% addition, Astrocyes are generally not found un-coupled in the brain, further
%% supporting the significance of astrocyte networks \parencite{postnov_2009}. The
%% specific molecules that generally diffuse across gap junctions are \ipt and
%% \ca, with only \ipt being modeled in some cases \parencite{pitta_2012}. In any
%% case the effect is generally described as excitable, regardless of the
%% molecule that diffuses \parencite{gordleeva_2021, pitta_2012, postnov_2009}.

\section{Astrocyte Model Development}
Figure \ref{fig:1n1s1a_fn_diagram} outlines the proposed astrocyte-\gls{lif} neuron
model, as well as the internal dynamics of the neuron.

\asvgf{figures/1n1s1a_diagram.svg}{Astrocyte and neuron model diagram for an \gls{lif}
  neuron with one input}{fig:1n1s1a_fn_diagram}{0.9}

It should be noted, that the astrocyte response pathways are similar in form to
an \gls{lif} neuron's membrane voltage. This decision is driven in part by a
simplification on some bio-realistic models, but also to demonstrate that key
properties of astrocytes are dependent on the pathways and relative
time-constants, less than the specifics of \ca, \ipt, or \kp dynamics.

\begin{align}
  d_{ip3} = d_t (-IP_3)\tau_{ip3} + \alpha_{ip3} z_{pre}(t) \label{eq:astro:spike-ip3} \\
  d_{k+} = d_t (-K^+)\tau_{k+} + \alpha_{k+} z_{post}(t) \label{eq:astro:spike-k+}
\end{align}

Where $\tau_{ip3/k+}$ represents a time constant for a given pathway, $z_{pre}$
is $1$ if there is a pre-synaptic spike at time $t$ ($0$ otherwise), and
$z_{post}$ is $1$ when the post-synaptic neuron fires ($0$ otherwise). For all
simulations (unless otherwise specified) $d_t = 0.001$ or $1ms$.

\begin{align}
  K^+ =
  \begin{cases} 
    d_{k+} & reset_{k+} = 1, z_{pre}
    \\ K^+ + d_{k+} & otherwise
  \end{cases} \\
  IP_3 =
  \begin{cases} 
    d_{ip3} & reset_{ip3} = 1, z_{post}
    \\ IP_3 + d_{ip3} & otherwise
  \end{cases}
\end{align}

\begin{align}
T_{delta} = -\alpha_{ltd} z_{pre} K^+ + \alpha_{ltp} z_{post} IP_3\\
d_{ca} = -\cam \tau_{ca} d_t + T_{delta} \label{eq:astro:temp-u}
\end{align}

In this case, $z_{post}$ and $z_{pre}$ are pre and post-synaptic spikes that the
astrocyte is sensing at a synapse. The $\alpha_{ip3/k+}$ parameters are
used to control the magnitude of influence \ipt and \kp have on astrocyte
\ca. In addition, $\alpha_{ltd/ltp}$ can override default \gls{ltp}/\gls{ltd}
behavior, implementing anti-stdp, or biasing weight updates towards
\gls{ltp}/\gls{ltd}. Note also, that \eq{eq:astro:temp-u} implies astrocyte \ca
will decay with each time step, but only be influenced by \ipt and \kp when
spikes occur, and not continuously. If enough \ca accumulates from coordinated
spiking events, a threshold $thr_{ca}$ is reached and \ca reset.

%%  Equations
%% \ref{eq:astro:temp-sign-1} and \ref{eq:astro:temp-sign-2} show definitions
%% of $sign_{ltd}$ and $sign_{ltp}$ that would allow a region around $dt=0$
%% where weight updates were 0.

%% \begin{align}
%% sign_{ltd} = -(1 - H(k+ + thr_{ltd})) \label{eq:astro:temp-sign-1}\\
%% sign_{ltp} = 1 - H(ip3 + thr_{ltp}) \label{eq:astro:temp-sign-2}
%% \end{align}

\subsection{Generalization of STDP}

The astrocyte model that has been outlined thus far is well-placed, and
well-defined to implement \gls{stdp} and \gls{stdp}-like local learning
rules. Through the \gls{ipt} and \gls{kp} pathways, there is a distinction
between the timing of pre and post-synaptic spikes. The only missing piece is to
inspect the \ca response and define a suitable $\Delta W(\cam)$. Figure
\ref{fig:stdp_dw_dt_impl} shows the \ca response of the astrocyte model, which
matches exactly the classic \gls{stdp} weight update curve. Using $\Delta
W(\cam)=\cam$, resetting \ca each time the weight is changed implements classic
\gls{stdp}.

\asvgf{figures/artifacts/obj1/1n1s1a_tp_l-stdp_w-dw_mult_dwdt_astro_plasticity.svg}{Simulation
  of astrocyte implementation of classic \gls{stdp}}{fig:stdp_dw_dt_impl}{0.8}

The astrocyte model is capable of going beyond classic \gls{stdp}, by
implementing some common variations found throughout the literature. These are
outlined in Table \ref{table:astro_varient_params}.

\begin{table}[htbp]\centering
\caption{Model parameters associated with \gls{stdp} variants.} \label{table:astro_varient_params}
\scriptsize
\begin{tabular}{lrrrrrrr}\toprule
\gls{stdp} Variant &$\alpha_{ip3}$ &$\tau_{ip3}$ &$\alpha_{k+}$ &$\tau_{k+}$ &$\tau_u$ &$thr_{ltp}$ &$thr_{ltd}$ \\\midrule
Classic &1 &100 &1 &100 &10000 &0 &0 \\
\Gls{ltd} Bias &1 &100 &1 &30 &10000 &0 &0 \\
\Gls{ltp} Bias &1 &30 &1 &100 &10000 &0 &0 \\
\Gls{ltd} Shift &1 &100 &1 &100 &10000 &0.5 &0.5 \\
\Gls{ltp} Shift &1 &100 &1 &100 &10000 &-0.5 &-0.5 \\
Anti-STDP &-1 &100 &-1 &100 &10000 &0 &0 \\
\bottomrule
\end{tabular}
\end{table}

In addition to generalizing \gls{stdp} variations that have been explored in
other works, this astrocyte model can leverage the flexibility of its \ca
response to improve learning. Introducing temporal integration, through addition
of a simple threshold on \ca, which gates the existing $\Delta W(\cam)$
operation. Figure \ref{fig:ppi:impulse_istp} shows learning progression with
this \ca threshold, and Figure \ref{fig:ppi:impulse_stp} shows the behavior of
classic \gls{stdp}.

\asvgf{figures/artifacts/obj2/snn_1n1s1a_tp_pulse_astro_0.svg}{
  Astrocyte response to successive spikes, displaying temporal pulse-pair
  integration}{fig:ppi:impulse_istp}{0.8}

\asvgf{figures/artifacts/obj2/snn_1n1s1a_tp_pulse_classic_0.svg}{
  Classic \gls{stdp} response to successive spikes}{fig:ppi:impulse_stp}{0.8}

Since weight changes are now based on multiple pre and post-synaptic spike
pairs, integrated into a \ca response over time, the learning is more stable,
and exhibits convergence, when compared to classic \gls{stdp}.

%% \subsection{Temporal Integration}

%% The process to trigger that external response is
%% thought in many works to be \gls{cicr}. Once that \ca concentration exceeds a
%% threshold, there is a cascade of events resulting in both \ca propagation to the
%% cell body, and the release of \gls{gt} into the synaptic cleft. This concept of
%% temporal integration, with \gls{cicr} as the mechanism fits well into the
%% current model, and was explored in this chapter as the mechanism that drives
%% weight change.

%% In the context of this work's astrocyte model, as \ca values increase and exceed
%% a threshold $thr_{ca}$, a weight change will occur at the synapse, and the \ca
%% value is degraded to zero, representing the re-uptake of \ca observed with
%% \gls{cicr} in biology. Tuning this threshold value led to some improvements for
%% both weight convergence given a set of inputs, and tolerance to overall noise in
%% the system.

%% \begin{align}
%%   d_w = w (\alpha_{ltp} H(\cam - thr_{ca}) + \alpha_{ltd} H(-\cam - thr_{ca}))  \label{eq:dw-thr}
%% \end{align}

%% \eq{eq:dw-thr} describes the baseline concept for weight
%% change. $\alpha_{ltp/ltd}$ parameters serve as learning rates, and can be tuned
%% based on the type of inputs or astrocyte performance.

%% Figure \ref{fig:astro:ramp_impulse} shows the behavior of the temporal integration
%% model when a series of spike impulses are presented to the network. Each
%% successive pulse has an additional spike from the last and the resulting \ca
%% activity is depicted for a given synaptic weight. This simulation exposes the
%% averaging effect of the spike timing model. With an impulse of 4 spikes, the
%% neuron is driven enough to output a single spike, this happens around
%% 300ms. With 4 pre-synaptic spikes and 1 post-synaptic spike, the astrocyte's \ca
%% is increased, exceeding the threshold of $2.5$. Subsequent spike impulses have
%% $>4$ spikes, which leads to a drop in \ca, due to anti-causal pairing
%% (transiently) of pre-synaptic spikes. Regardless of these transient drops, \ca
%% consistently exceeds $thr_{ca}$ throughout the
%% simulation.

%% \asvgf{figures/artifacts/obj1/astro_tp_impulse.svg}{Astrocyte response to
%%   ramping impulse input: demonstrating pulse-pair averaging
%%   feature}{fig:astro:ramp_impulse}{0.5}

%% Figure \ref{fig:ppi:impulse_istp} depicts the astrocyte plasticity model with
%% $Ca_{thr}=2.5$, $\alpha_{ltp}=1.05$ and $\alpha_{ltd}=0.95$. In contrast, Figure
%% \ref{fig:ppi:impulse_stp} shows the behavior of a classic \gls{stdp} (implemented by
%% the astrocyte) with the same inputs and time-constants. The inputs themselves
%% consist of successive spikes, with 1ms between each. These spikes form groups
%% with enough time between for neuron and astrocyte states to decay. Each group
%% has an additional spike compared to the last.

%% Comparing these graphs, it is clear that integrating behavior of the astrocyte
%% model effects the learning process significantly. Resulting in more stable
%% behavior, and increasing synaptic weight due to causal, correlated inputs. The
%% key effect here can be seen considering what happens when there are additional
%% input spikes after a post-synaptic spike. In these cases classic \gls{stdp}
%% would decrease the weight, while the astrocyte integrates this event into
%% \ca. Those input spikes are eventually associated with an additional down-stream
%% spike, forming a causal relationship and \ca is increased. Overall this leads to
%% \gls{ltp}, with the potential \gls{ltd} events being smoothed out.

%% \asvgf{figures/artifacts/obj2/snn_1n1s1a_tp_pulse_astro_0.svg}{
%%   Astrocyte response to successive spikes, displaying temporal pulse-pair
%%   integration}{fig:ppi:impulse_istp}{0.6}

%% \asvgf{figures/artifacts/obj2/snn_1n1s1a_tp_pulse_classic_0.svg}{
%%   Classic \gls{stdp} response to successive spikes}{fig:ppi:impulse_stp}{0.6}

%% The smoothing effect observed with the astrocyte plasticity model should work
%% just as well at smoothing out noise, as it does transient causal or anti-causal
%% spike pairings. Testing this, Figures \ref{fig:ppi:impulse_noise_stp} and
%% \ref{fig:ppi:impulse_noise_istp} show the results of similar simulations, with
%% the addition of uniformly random noise in both cases. The probability specified,
%% is the probability that a noisy spike is received at the pre-synaptic
%% terminal during any single time-step. Observing the weight updates associated
%% with the astrocyte model, there isn't a significant impact from the noise on the
%% progression of weight modulation. In the case of classic \gls{stdp}, weight values
%% fluctuate even further, adding to the already unstable behavior.

%% % STDP response to impulse + uniform noise
%% \asvgf{figures/artifacts/obj2/snn_1n1s1a_tp_pulse_const_noise_classic_0.svg}{Classic
%%   \gls{stdp} response to impulse inputs with uniformly random
%%   noise}{fig:ppi:impulse_noise_stp}{0.6}

%% % Astro response to impulse + uniform noise
%% \asvgf{figures/artifacts/obj2/snn_1n1s1a_tp_pulse_const_noise_astro_0.svg}{Astrocyte
%%   response to impulse inputs with uniformly random
%%   noise}{fig:ppi:impulse_noise_istp}{0.6}

%% \subsection{Local Convergence Points}

%% Figure \ref{fig:astro:tp_many_w_sweep} graphs the number of times $thr_{ca}$ is
%% exceeded during a fixed length simulation, for a few different values of
%% $thr_{ca}$. For $\cam=2.5$, regions of the graph around $0.8$ and $1.2$ show
%% weight values that result in a stable configuration, where all \ca activity is
%% sub-threshold. Figure \ref{fig:astro:tp_many_w_tl} shows
%% a timeline of astrocyte activity for three different weight values, and
%% $thr_{ca}=2.5$.

%% \asvgf{figures/artifacts/obj1/astro_tp_many-w_sweep.svg}{Aggregate astrocyte
%%   response to input from a single synapse, with various synaptic weight and
%%   $thr_{ca}$}{fig:astro:tp_many_w_sweep}{0.6}

%% \asvgf{figures/artifacts/obj1/astro_tp_many-w_xlim_tl.svg}{Timeline of astrocyte
%%   activity with input from a single synapse, given
%%   $thr_{ca}=2.5$}{fig:astro:tp_many_w_tl}{0.6}

%% This concept of sub-threshold oscillations is important, as it provides a
%% mechanism for astrocyte-mediated plasticity to reach a convergence point given a
%% set of inputs. This convergence point is dependent on the input pattern in
%% question, and the $\Delta \cam$ vs $\Delta t$ curve characterizing the \ca
%% response. Considering Figure \ref{fig:astro:tp_many_w_tl} and recalling the
%% shape of Figure \ref{fig:astro:classic_stdp}: The three output spikes align with
%% input spikes, which maximizes $\Delta t$ between the output spikes, and
%% neighboring presynaptic spikes in any direction. This maximal spacing leads to a
%% minimum \ca response. Note that spikes with $\Delta t=0$ do not elicit a \ca
%% response. Independent of the weight value \ca remains sensitive to activity at a
%% synapse. Changes in the input firing pattern may result in \ca oscillations
%% exceeding the threshold once more, and trigger another round of weight
%% updates. In this way, astrocytes can converge on synaptic weight values, but
%% still support continuous learning.

\section{Multi-Synapse Astrocyte Plasticity}

In biology, astrocytes are observed influencing many synapses. In general, they
interact with a single digit number of neurons and up to 100,000 synapses. This
spatial integration is a fundamental property of astrocytes, and the response to
activity from these connected synapses is observed as a cell-level \ca
response. Taking this concept and applying it to the astrocyte model in this
work, a coordinated approach to synaptic plasticity is developed. This approach
is employed in a learning task where a multi-synapse view is critical.

\subsection{Synaptic Coupling}
This section introduces the concept of synaptic coupling. It is a regional
response, which occurs between the strictly local response that has been
explored thus far, and the slower (seconds timescale) cell-level responses
directly observed in biology. The response is fast (unlike the cell-level \ca
responses), operating at the speed of the local responses explored thus far, and
encompasses a hand-full of synapses. Some in the computing and neuroscience
communities believe similar intermediate astrocyte responses occur in nature,
but that researchers don't have the capabilities to observe them yet. In any
case, synaptic coupling fits the common theme of multi-level integration, and is
shown to be an effective approach in an example learning task. Figure
\ref{fig:astro:syn_coupling} shows the progression of the astrocyte model from
exhibiting a strictly local responses, to support synaptic coupling. There are
three major changes to the model from previous iterations.

\begin{enumerate}
\item The \ca response generated local to a synapse propagates up to the
  regional level.
\item At the regional level, there is some function implemented by the
  astrocyte, which directs plasticity across associated synapses.
\item At the regional level, two additional internal signals \dser and
  \serca are introduced. These can be though of as chemical signals which can
  propagate from the regional level, down to a given synapse. They are
  responsible for triggering a weight change locally, or causing
  degradation/re-uptake of \ipt, \kp, and \ca.
\end{enumerate}

\asvgf{figures/local-multi-compare.svg}{Astrocyte functional diagram for
  multi-synapse synaptic coupling}{fig:astro:syn_coupling}{1.0}

\subsection{Synaptic AND Coupling}
The function explored for multi-synapse astrocyte plasticity is logical
AND. This is implemented by a single astrocyte, influencing N synapses of a
single \gls{lif} neuron. The expected behavior of an \gls{lif} neuron
implementing AND can be defined in terms of single spikes as shown in Figure
\ref{fig:global-v-local-and-coupling}.

In Figure \ref{fig:global-v-local-and-coupling} each of the blue or orange boxes
represents a spike, either pre-synaptic, or post-synaptic. Figure
\ref{fig:global-v-local-and-coupling} shows the difference between the local
astrocyte response, vs. the regional response required to implement logical
AND. Without any context of the response from other synapses, a strictly local
rule isn't able to correctly converge to an AND function. In most situations
outlined, the regional control logic will need to signal the correct response to
one or more synapses.

To implement this, first the typical activity local to a synapse is considered,
These \ca responses propagate, and are readily available at the regional
level. Using the \ca transients from each synapse, the global logic determines
if weights should change on a given synapse, and if so, in what direction. To
communicate the proper response \dser (which triggers a weight change at a
synapse) and \serca, (which triggers messenger degradation and re-uptake)
signals are passed from the regional control level, to individual synapses.

\asvgf{figures/AstroAndCoupling_implementation.svg}{Comparison of astrocyte
  response locally, and the desired and coupling
  response}{fig:global-v-local-and-coupling}{1.0}

%% \asvgf{figures/AstroAndCoupling_signals.svg}{Logical definition for AND
%%   coupling with \ca signaling}{fig:local-ca-global-response}{1.0}

In essence, all spiking activity on N synapses (for the purpose of implementing
AND) fall into one of the following categories.
\begin{itemize}
\item AND - The neuron behaved correctly, this is either $pre_0, pre_1, ...,
  pre_n \implies post_o$ or $others -> \neg post_0$ : Reset synapse state, \ipt,
  \kp, and \ca cleanup.
%% \item Other-influence - This situation occurs when $post_o$ comes before all
%%   $pre_i$ inputs. This implies something happening outside the control of
%%   associated synapses. It doesn't make sense to change the weight in this case,
%%   as it is unknown why there was a $post_0$ spike. In addition, the \gls{lif} neuron
%%   may be in a refractory period, and it wouldn't make sense to try and update
%%   weights during that: Reset all synapses.
\item Early-Spike - $post_0$ occurs before some spiking inputs, but after
  others. This would indicate one or more of the input synapses have weights
  that are too high: \gls{ltd} on synapses that received inputs.
\item LPT - If input spikes arrive at $pre_0, pre_1, ..., pre_N$ and there is no
  post-synaptic spike $post_0$, this implies weight values are too low: trigger
  \gls{ltp} on all synapses.
\end{itemize}

%% Which of these categories a given set
%% of inputs and response belongs to, depends on some model parameters. Namely,
%% the timing constants and thresholds $thr_{and}$ and $thr_{ltp}$. Given a
%% set of parameters, there is a deterministic window of time spikes must fall into
%% in order to be considered for a particular response. Outside this window, from
%% the perspective of the astrocyte, those spikes never happened. This window is
%% dependent on many parameters however, and possibly changes given previous
%% activity. Instead of solving for the exact point in time a spikes falls out of
%% the time window, an upper bound is defined, $\delta_{ptp}=10ms$. The astrocyte
%% was tuned to guarantee proper behavior if spikes fall within this
%% window. This implies that the time between the first pre-synaptic spike, to the
%% first (possibly only) post-synaptic spike must be no more than this value. With
%% this timing constraint a variety of pre-pre-post spiking events can be generated
%% in a two-synapse configuration. This experiment will remove the influence of
%% synaptic weights and \gls{lif} neuron dynamics, focusing solely on astrocyte
%% response \ca. Running this simulation, it was found that the astrocyte responded
%% correctly to inputs 100\% of the time, if the astrocyte state was reset before
%% presenting each test input.

In order to constrain the requirements, and allow for tuning of astrocyte
parameters, a limit $\delta_{ptp}=10ms$ was defined. This defines a sliding
window of time, where the astrocyte is tuned to respond. It was shown that with a
zero initial state, all possible spike configurations in a 10ms window were
handled correctly. Real-world applications will involve a set of continuous
inputs however, and spikes arrive with the astrocyte in a variety of
states. With this additional variable there are some situations where the
astrocyte does not respond as expected (a miss-match from ground truth), in
general this is due to a \ca deviation from zero due to previous activity. These
mismatches made up $\approx 14\%$ of 10,000 10ms bouts presented to the
astrocyte. When considering $n=3$ and $n=4$ synapses with continuous randomly
generated spiking inputs, the error rate observed is consistent with that of
$n=2$. For $n=3$ there are 283 mismatches out of 2000 10ms bouts (14.15 \%
error). For $n=4$, there are 416 mismatches out of 3000 (13.9 \% error).

%% By default there are some cases present in the randomly generated spike train
%% that are impossible in the current configuration. Namely, cases where a
%% post-synaptic spike precedes all pre-synaptic spikes. This would be possible
%% with influence from other synapses outside of the AND-coupling domain, or
%% through the introduction of noise into the neuron model, but those cases aren't
%% considered here. With the removal of such cases, the error rate increases to
%% 23.3 \% for 2 synapses, 22.0 \% for 3 synapses, and 18.0 \% for 4 synapses. The
%% increased error rate indicates that they astrocyte model benefits from sparsity
%% in activity needing a response.

%% Initial simulations which have isolated the astrocyte and provided random
%% inputs, indicate that in a majority of cases, the astrocyte is capable of
%% directing plasticity correctly to implement an AND function. A fairly consistent
%% error rate of between 14 \% and 24 \% is observed, which is theorized to be low
%% enough for proper convergence to implement AND.

%% Closing the loop fully, the weight update behavior outlined in
%% \eq{eq:astro_and_dw} is implemented. With that addition, an astrocyte-\gls{lif}
%% (1n2s1a) configuration was simulated, given the same continuous inputs generated
%% by concatenating 10ms bouts, but only of pre-synaptic spikes for $n=2$
%% synapses. Where before, post-synaptic spikes were also randomly generated, now
%% any post-synaptic activity is driven by an \gls{lif} neuron wih synaptic
%% weights. The initial synaptic weight is $\approx 0.7$.

%% \begin{align}
%%   dw_{syn} = ca_{syn} lr_{ltp} \textrm{ if } ca_{syn} >
%%   0 \label{eq:astro_and_dw} \\
%%   dw_{syn} = ca_{syn} lr_{ltd} \textrm{ if } ca_{syn} < 0 \\
%% \end{align}

\asvgf{figures/artifacts/obj3/astro_and_lif.svg}{Weight values and error rates
  during learning in 2, 3, and 4 synapse
  configurations}{fig:snn_astro_and}{1.0}

Looking at the top two plots in Figure \ref{fig:snn_astro_and}: Initially,
synaptic weight values are too high, and the neuron fires in response to a
single pre-synaptic spike. As expected, the astrocyte triggers \gls{ltd}, and
the synaptic weight decreases. This continues, and the \gls{lif} neuron begins
to fire in response to two pre-synaptic spikes. There was a slight
over-correction, and some \gls{ltp} events occur, bringing the weights back
up. Eventually, the synaptic weights converge and offer a good approximation of
an AND function over the inputs.

Looking at the second plot, there are two error rate traces, labeled
$+\Delta w$ (blue) and $-\Delta w$ (orange). These indicate example inputs where the neuron
responded incorrectly, and one or more synaptic weights needed to be increased,
or decreased respectively. Summing these two error rates give an overall error
rate.

Initial simulation with $n=2$ demonstrated success for coordinated astrocyte
plasticity. Testing the learning approach further, lower plots show similar
simulation with 3 and 4 synapse configurations. In each case, the addition of a
synapse beyond 2 results in increased noise and oscillations in the weight
values towards the end of simulation. Though still moving in the right
direction, the astrocyte plasticity approach provides results with significant
error. Looking at the mean error rate towards the end of simulation, it is
upwards of $20\%$. In order to better explain what is going on, consider the
error rate plots. There is an oscillation, between the majority of errors
involving $+\Delta W$ (blue, weight values are too low) and $-\Delta W$ (orange,
weight values are too high). This indicates that optimal weight values are being
under and overshot repeatedly. Oscillation which prevents proper convergence is
a common problem in learning rules, and is not a fundamental drawback. Though it
wasn't explored here, some reasonable next steps would be to introduce a
learning rate schedule, or to add the concept temporal integration at the
regional level. Simply lowering the overall learning rate lead to better
convergence, but with a higher error rate than observed with the noisy solution.

To ensure the learning rule is robust to weight initial conditions the starting
values were uniformly distributed from $0.0$ to $2.0$, instead of all being
fixed to $0.7$. Figures \ref{fig:snn_4s1a_and_w_dw_w} and
\ref{fig:snn_2s1a_and_w_dw_w} show learning progression for two simulations,
which indicate the initial weight value does have some effect on learning, but
overall convergence was unaffected, and the error rates are approximately the
same as previous runs.

\asvgf{figures/artifacts/obj3/snn_2s1a_and_w.svg}{Weight change in the 2s1n1a
  configuration, with random initial weight values between $0.0$ and
  $2.0$}{fig:snn_2s1a_and_w_dw_w}{1.0}

\asvgf{figures/artifacts/obj3/snn_4s1a_and_w.svg}{Weight change in the 4s1n1a
  configuration, with random initial weight values between $0.0$ and
  $2.0$}{fig:snn_4s1a_and_w_dw_w}{1.0}


\section{Conclusion} \label{section:conclusion}
The goal of this work was to advance the capabilities of \glspl{snn} using a
computational astrocyte model, geared towards synaptic plasticity. Key
properties of astrocyte-neuron interaction were identified from biology, and
leveraged in a simplified computational model. The intersection of signaling
pathways and functional roles that best fit this work was determined to be
synaptic plasticity, with a focus on multi-level integration across time and
space. A computational astrocyte model was proposed, and shown to be capable of
implementing classic STDP, as well as common variants found in the existing
literature. Taking advantage of the \ca response and temporal integration, the
astrocyte demonstrated improvements over \gls{stdp} in a single-synapse
configuration, with a given test input. The astrocyte model was extended to
support integration of activity from multiple synapses and the concept of
synaptic coupling was introduced. Using logical AND as a test function, it was
demonstrated that a multi-synapse learning approach could direct \gls{lif}
neuron weight values to a solution for logical AND. This was demonstrated in 2,
3 and 4 synapse configurations, with lesser degrees of success with each
additional synapse. This work lays the groundwork for an astrocyte-like approach
to synaptic plasticity, with an emphasis on coordination spatially and
temporally, breaking away from a strictly local view. It provides this with a
computationally simple, bio-inspired approach.

% Add the acronyms
%% \glsaddall
%% \printglossary[type=\acronymtype]

%% % Reset all acronyms
%% \glsresetall

%% \renewcommand*{\bibfont}{\footnotesize}
\renewcommand*{\bibfont}{\small}

% Bibliography file
\printbibliography[heading=bibintoc]


\end{document}
