% objective_details.tex
%
% Author       : Jacob Kiggins
% Contact      : jmk1154@g.rit.edu
% Date Created : 01/31/2021
%

% NOTE: All filler text has "TODO" written. This must be removed in the final copy!

\chapter{Methods and Resources} \label{section:methods}
    The overarching goal of this thesis is to extend the bio-inspiration of
    spiking networks by introducing an astrocyte-like element, and forming a
    tripartite synapse. This astrocyte element will drive synaptic plasticity;
    first implementing, then extending the widely used \gls{stdp} rule. Benefits of
    astrocyte-like control of synaptic plasticity will be demonstrated in the
    single synapse, single neuron configuration (1S1N) as well as select
    multi-synapse configurations (i.e. 2S1N, 3S1N, etc...). Inputs mostly
    consisted of procedurally generated spiking inputs, including Poisson
    rate-coded spike trains, spiking inputs representing Boolean variables, and
    temporal inputs that follow specific patterns. In general, for each
    experiment there will be some comparison necessary between the astrocyte
    driven plasticity and classical \gls{stdp}. To evaluate each approach
    convergence/divergence of weights or speed of convergence may be
    used. Statistical analysis of various intermediate signals, or the weights
    may be useful as well, depending on the experiment. If it is difficult to
    compare an experiment to classical STDP, it should instead be possible to
    compare to another astrocyte driven experiment.

    %%%%%%%%%%%%% Datasets %%%%%%%%%%%%%
    \section{Datasets} \label{section:datasets}
    No formal dataset is used in this research. Instead, inputs to spiking
    neurons and spiking neural networks are simulated using rate-based and
    time-based spike-trains. These spike trains have specific patterns and are
    chosen based on the experiment at hand. Some example pattern include.
    \begin{enumerate}
    \item Random Poisson spike train with a single rate parameter
    \item Random Poisson spike train with a variable rate parameters
    \item A bust of spikes which may be inserted into another spike train
    \item Spikes representing Noise: Uniformly random, Gaussian, etc...
    \item Specific temporal patterns either hand-picked, spatial, or driven by a
      boolean function
    \end{enumerate}
        
    %%%%%%%%%%%%% Metrics %%%%%%%%%%%%%
    \section{Metrics} \label{section:metrics}
    Few of the common metrics used in machine learning will be applicable for
    this research. Instead results will be presented using comparisons between
    plasticity variants, where the methods in this research showed some
    improvement. In addition, results may display features not previously
    observed. In that case novelty will be the metric. Accuracy will likely
    still be applicable, especially with supervised variants of the astrocyte
    plasticity model.
    
    Two metrics will be used to monitor the network from a more
    application-independent perspective. This will give insight into how
    different architectures are behaving at a low level. The first is a
    per-neuron spike-trace. This will provide information like average spike
    rate, and show any areas of the network with increased activity. From this
    trace a higher-level insights can be made, such as counting the number of
    poly-synchronous groups \parencite{gaetano_2013}.
    
    %%%%%%%%%%%%% SW Components %%%%%%%%%%%%%
    \section{Software Components}
    \Glspl{snn} are very much in their infancy as far as software and community
    support. This isn't due to lack of options, many frameworks are capable of
    simulating SNNs. However, these frameworks aren't as sophisticated,
    complete, error-free, or supported when compared to deep learning
    frameworks. In addition, many of the more bio-realistic frameworks can't
    simulate networks at scale, such as NEST \parencite{nest}. Recently, a few
    python libraries have emerged that are usable and very close to complete
    solutions. The first is BindsNet \parencite{bindsnet}, which has been
    referenced in a handful of papers. The original implementation included an
    architecture that achieved \acrshort{sota} results on \gls{mnist}
    classification. The second is PySNN, it was written by a PhD student, and
    builds on-top of PyTorch. Many common Spiking Neuron Models are included out
    of the box, such as \gls{lif}, and the more bio-inspired Izhikevich
    model. Another framework, somewhat newer is Norse. Unlike others listed
    here, Norse is in active development. Similar to PySNN, Norse provides a
    very clean interface and is built ontop of PyTorch. Specifically Norse
    leverages PyTorch's torchscript interface, which allows Norse to claim (and
    in some cases defend) that they have the most efficient library for
    simulation of \gls{snn} \parencite{norse2021}.

    This research leveraged a modified version of Norse for all simulations.
    
