% background.tex
%
% Author       : Jacob Kiggins
% Contact      : jmk1154@g.rit.edu
% Date Created : 01/09/2020
%

% NOTE: All filler text has "TODO" written. This must be removed in the final copy!

\chapter{Background and Related Work}\label{chapter:background}
    \section{Spiking Neural Networks}
    \glspl{snn} are a type of brain-inspired computing model. This class of
    \gls{ann} is the most bio-realistic that is used in engineering
    applications. They are an important topic in the machine learning field
    specifically, and have gained interest in recent years. Perceptron type
    neurons are similar to spiking neurons, but differ in a few key ways. Both
    types of neurons have some number of synapses, on which inputs are
    received. These inputs are transformed based on per-synapse parameters
    (generally multiplied by a synaptic weight), then summed together. This is
    where the behavior diverges. \Gls{sn} add to a leaky membrane voltage with
    each input. If this membrane voltage satisfies an activation function
    (generally a voltage threshold) then the \gls{sn} fires, or generates an
    output spike. For perceptron \glspl{an}, there is still an activation
    function, and that function may produce a spike-like output (1 or 0), but
    can output any real value. Similarly on the input side, \glspl{sn} receive
    spikes on their synapses, where \glspl{an} can receive any real
    value. Another primary difference is related to time: A \gls{sn}'s membrane
    voltage persists over time, unlike \gls{an}'s, where there isn't a
    per-neuron state maintained across inputs.
    
    \glspl{snn}, formed by connections between spiking neurons, are a
    bio-realistic type of \gls{ann}. \gls{sn} instead emit spikes,
    reminiscent of biological neuron's action potentials. These spikes travel
    across a synapse, from one neuron to another, and are usually multiplied by
    a weight along the way. In some cases, there are more complex dynamics
    involved at the synapse, including delay, filtering, or the modeling of
    diffusion. At the downstream neuron's post-synaptic terminal, spikes arrive
    and are transformed into a voltage wave, called a \gls{psp}. \glspl{psp} are
    added over space and time, and accumulate at the neuron downstream, in the
    form of membrane voltage, or more generically a state variable. An
    activation function of this state variable determines if and when a spiking
    neuron fires. Generally, once a certain threshold is reached the
    post-synaptic neuron emits a voltage spike (fires). Since there are many
    Spiking Neuron models, it is necessary to formalize what is considered a
    spiking neuron. In general they process information from one or more inputs,
    and produce a single spike-like output. The probability of firing is
    increased by excitatory inputs, and decreased by inhibitory inputs. There is
    at least one internal state variable, and depending on this variable output
    spikes are generated \parencite{ponulak_2011}.
    
    There are many potential models that fit the requirements of a spiking
    neuron. In addition, some models are more often paired with other
    bio-inspired components such as astrocytes (explored in this work). Relevant
    models are outlined below. These models each balance bio-realism with
    computational efficiency, with some excelling at both. Figure
    \ref{fig:sn_model_compare} shows where a number of models fall when measured
    by these criteria \parencite{izhikevich_2004}. Specifically, the X-axis
    shows the number of floating point operations per-second (FLOPS) are
    required to simulate the model in real-time. In the Y-axis counts the number
    of cortical ``features'' out of a total of $22$ that model is able to
    reproduce.
    
    \afigf{figures/sn-model-compare.png}{Comparison of spiking neuron
      models}{fig:sn_model_compare}{1.0}

    %%%%%%%%%%%% Spiking Neuron Models %%%%%%%%%%%%
    \subsection{Leaky Integrate and Fire Neurons}
    Due to it's simplicity and computational efficiency the most popular
    \gls{sn} model explored in the literature is the \gls{lif} neuron. In this
    model there is a single state variable represented by an internal voltage,
    which increases as spikes arrive at the post-synaptic terminal of the
    neuron. Charge leaks from this internal reservoir over time, lowering the
    voltage. \eq{eq:lif} defines the behavior of a generic \gls{lif} model.

    \begin{align}
        \frac{du}{dt}(t) = -\tau_{mem}u(t)+(i_o + \Sigma w_jz_j(t)) \label{eq:lif}
    \end{align}
    
    In \eq{eq:lif}, $\tau$ is the leaking time-constant, with $i_o$
    representing a constant leak, independent of $u$. $\Sigma w_ji_j(t)$ is the
    increase in $u$ at time $t$, given the presence of a spike $z_j$ on synapse
    $j$, and the synaptic weight $w_j$. Once the state variable u (generally
    thought of as a voltage) reaches a certain threshold the neuron outputs a
    spike. After this time the state variable is reset, and is held at that
    value for a period known as the absolute refractory period. This mimics the
    high level behavior of biological neurons \parencite{ponulak_2011}.

    %% \todo{Add \gls{lif} diagram}

    %% \todo{Add 2 tau model from Norse}

    \subsection{FitzHugh - Nagumo Model}
    The FitzHughâ€“Nagumo is a common \gls{sn} model that was coupled with
    astrocytes in a number of works \parencite{postnov_2009, postnov_2007}. It
    is based on the highly (computationally) complex Hodgkin-Huxley model, with
    a variety of parameters fixed and others tuned to get different firing
    patterns. The model consists of two main components, the synapse represented
    by $w_i$, and the post-synaptic neuron, with membrane voltage $v_i$. If this
    membrane voltage exceeds a threshold $h_s$, then the neuron fires. 

    \begin{align}
      \epsilon_1 \frac{dv_i}{dt} = v_i - \frac{v_i^3}{3} -
      w_i \label{eq:fn_neuron}  \\
      \frac{dw_i}{dt} = v_i + I_i - I_{app} \\
      \tau_s \frac{d_z}{dt} = (1 + tanh(S_s(v_i - h_s)))(1 - z) - \frac{z}{d_s} \label{eq:fitz_nn}
      \\
      I_{syn} = (K_s - \delta G_m)(z - z_0)
    \end{align}

    The specifics of this model aren't relevant here only the separation of
    synapse and neuron quantities $w_i$ and $v_i$, and the associated
    implications. This neuron model, and those that model current flow within a
    synapse were common candidates for astrorcyte integration among the
    literature, and have strong ties to biology. \parencite{wade_2011,
      pitta_2016, postnov_2009}.
    
    %% The constant $\epsilon_1 = 0.04$ is the time-separation parameter and $I_i =
    %% 1.02$ defines the operating regime as excitatory for neuron $i$. $H_s$ and
    %% $D_s$ control activation and relaxation of the neuron. $I_{app}$ represents
    %% the input to the synapse at all sources. To control the post-synaptic
    %% behavior, dynamics of $z$ are defined, which represents a spike from the
    %% neuron $z \approx 0$ or $z \approx 1$. The spike $z$ induces a current at
    %% the post-synaptic terminal, given by $I_{syn}$.

    %% \todo{Add Image of simulation postnov 2009 pg 492}
    %% \todo{Add sections for SRM and Izh}
    
    \subsection{Spiking Neural Network Topologies}
    \glspl{snn} can take on a variety of topologies, similar to
    \glspl{mlp}. These include feedforward, recurrent, or a hybrid of
    both. Within a standard feed-forward network, lateral inhibition is often
    employed, resulting in a winner take all (WTA) configuration. In this case a
    layer of neurons has feed forward connections from the previous layer, in
    addition to inhibitory connections within the layer. The inhibitory
    connections from the first neuron to emit a spike to its neighbors, prevents
    other neurons within that layer from firing in the near term
    \parencite{ponulak_2011}.
    
    One, more specific hybrid configuration is a Synfire chain, which is
    feedforward between sub-populations of recurrent networks. Synfire chains
    are of particular interest, as they show a coordination within the spiking
    network. From input to output in a feed-forward pattern, populations of
    neurons coordinate, passing along a "packet" of neuronal activity. It was
    observed in monkeys, that the precise timing of spikes was correlated with
    behavior, and synfire activity was given as a possible explanation
    \parencite{aertsen_1996}.
    
    %%%%%%%%%%%% Coding Schemes %%%%%%%%%%%%
    \section{Coding Schemes}
    Spiking neural networks, receive and transmit spikes, rather than continuous
    values as with other \glspl{ann}. These spikes are of a uniform shape and
    size, and are indistinguishable from one another. This led to important
    questions: how to encode real values as inputs to \glspl{snn}, and how is
    information passed within an \gls{snn} encoded. Generally, coding schemes
    used are based on observations in biology \parencite{ponulak_2011}.
    
    Rate-based coding is based on some of the earliest observations of sensory
    neuron activity. As pressure on a tactile nerve was increased researchers
    observed an increase in firing rate of that receptor neuron. This translated
    into rate-based coding in artificial \glspl{snn}
    \parencite{ponulak_2011}. Rate-based coding couldn't explain certain
    observations in biology however. Some responses were too fast for the
    neurons involved to estimate the firing rate. In addition, dynamic responses
    have been observed in the primary auditory cortex without a changing of
    firing rate, but instead, firing pattern within a sub-population of
    neurons. More specifically, the relative timing of two or more spikes can
    encode information. This is very advantageous, since a spike lasts $\approx
    10^-3s$, but relative timing between spikes has a resolution of $10^{-8}$ s
    \parencite{ponulak_2011}. This greatly increases the bandwidth of
    communication, and theoretical minimum latency. The general trend, is that
    any neurological system where processing speed is especially important, will
    tend to rely on a spike timing based encoding for information.

    A variety of temporal coding schemes have been proposed, and
    investigated. Time to first spike, as the name suggests, encodes information
    in the latency between stimulus and the first spike. This is accomplished by
    using a neuron model that has an inhibitory feedback connection, suppressing
    additional spikes. \gls{roc} represents information in the order of spikes
    within a population of neurons, that each emit a single spike. This allows
    fast information processing, since the time between spikes isn't important,
    and can be very small. This scheme was proposed as an explanation for
    ultra-fast processing in the primate visual cortex. With latency coding,
    information is represented in the order of spikes, and the precise timing
    between spikes in a population of neurons. As with \gls{roc}, each neuron
    emits a single spike. This encoding scheme is supported by evidence in
    biological networks, where it has been observed that moving the timing of a
    single spike by ~10ms can change downstream activity
    \parencite{ponulak_2011}.
        
    %%%%%%%%%%%% Encoding Real Values %%%%%%%%%%%%
    \subsection{Encoding Techniques}
    In general, data being presented to a \gls{snn} will take the form of real
    values. This is true of images, sound, values from various integrated
    sensors, such as temperature or acceleration. A method is needed that can
    convert these real values into a spike, or spikes, as defined by a coding
    scheme.
    
    \subsection{Poisson Encoding/Decoding}
    A very popular method of encoding real values into spikes, or just to
    generate random spikes, is treating spikes as a Poisson process. In general,
    the rate parameter is either random, or equal to the real value being
    encoded. There are two main approaches for sampling spikes from a Poisson
    distribution. The first calculates the probability of a spike in a given
    time-step $P(k)$, with the value to be encoded as a firing rate. The
    resulting probability 
    is compared to the result of sampling a uniform distribution sampled once
    for each discrete time step. If the uniformly random value is below the
    Poisson probability a spike $spike_i$ is inserted, at time $i$. One potential issue
    with this, relates to numeric precision. If the simulation time step is
    small in comparison with the rate parameter, then the probability of an
    event may be quite small. Go small enough, and the uniformly random number
    won't have enough precision to ever dip below the threshold. In this case no
    spikes would be produced.
    
    %% \todo{ref  https://www.cns.nyu.edu/~david/handouts/poisson.pdf}
    
    \begin{align}
        P(k) = \frac{\lambda^k e^{-\lambda}}{k!} \\ P(1) = \lambda e^{-\lambda} \\
        spike_i = P(1) > X_i \\
    \end{align}
    
    The next, and more numerically stable method, is to use the exponential
    distribution, or Poisson waiting time to sample intervals between
    spikes. Interval times are randomly sampled from the exponential
    distribution.
    
    \begin{align}
      x = rand(0,1) \\
      interval = \lambda e^{-\lambda x} \\
    \end{align}

    $interval$ is computed repeatedly until the desired length spike train has
    been constructed, with $x$ being a uniformly distributed random
    variable. The second approach outlined here is used in this work.
    
    %This technique generates a spikes train as shown in the following graph.
    
%    \afig{figures/encode_poisson_sweep.png}{Poisson encoded spike train for real values between 0 and 1}{fig:enc_poisson_sweep}
    
    To determine how different operations process data within a \gls{snn}, it is
    useful to be able to decode a spike train into real values. This involves
    finding the most likely $\lambda$ parameter for given spike train. Using
    maximum likelihood estimation this is found to be the inverse of the sample
    mean, where $N$ is the duration sampled, and $x_i$ is $0$ or ) $1$ if a
    spike occurred at time $i$.
    
    \begin{align}
        \lambda = \frac{N}{\Sigma x_i}
    \end{align}
    
    This decoding process produces an estimate of the firing rate, which
    improves with sample size.
    
    \subsection{Temporal Coding}
    There are a variety of temporal coding schemes. The main distinction from a
    rate-based approach, is the reliance on precise spike timing to encode
    information. Beyond a qualification of these methods, a precise definition
    (or definitions) need to be presented and used during simulation.
    
    \subsubsection{Rank Order Coding}
    Information is encoded in the relative spike times within a population of
    neurons. Encoding itself is fairly straight-forward. The real values that
    make up a single input (such as an image) are transformed into a
    latency. This latency defines the time until a spike is presented to a
    single synapse. Generally larger values are assigned a smaller latency, and
    smaller values a larger one \parencite{delorme_2001}. This procedure so far
    makes no distinction between \gls{roc} and latency coding. If the time
    between spikes were set to some fixed value, but spike order maintained then
    \gls{roc} would be satisfied but not latency coding.
    
    Some changes to the network itself have shown success when paired with
    \gls{roc}. These include a progressive desensitization, similar to the
    biological concept of shunting inhibition. \eq{eq:roc_activation} shows the
    activation of a neuron, given a modulation factor $\alpha \in (0,1)$ and a
    set of incoming connections $a_i$. Subsequent spikes along a given input
    connection have less and less of an effect on the overall activation of the
    neuron \parencite{delorme_2001}.
    
    \begin{align}
        Activation(i) = \Sigma_{j \in [1,m]}\alpha^{order(a_i)}W_{j,i} \label{eq:roc_activation}
    \end{align}

    Where $i$ is the synapse, $order(a_i)$ is the order of incoming spikes, and
    $\alpha$ is a modulation factor.
    
    
    %%%%%%%%%%%% Convolution in SNN %%%%%%%%%%%%
    %% \section{Convolutions With Spiking Networks}
    %% Convolution is not as straightforward to implement in the spiking domain,
    %% when compared to traditional ANNs. There are certainly many ways one could
    %% implement convolution. Including, in the same way they are handled in ANNs,
    %% where neurons aren't explicitly involved. However, the goal is to be
    %% biologically plausible. Observations in the human visual cortex inspired the
    %% tiling window approach to convolution \parencite{wang_2016}. Instead of applying
    %% an activation like ReLU, the activation is a spiking neuron. In addition,
    %% instead of defining convolution as a time-based operation, it is implemented
    %% in space, where the kernel is represented by same-valued sets of weights,
    %% connecting feature maps, represented by spiking neurons
    %% \parencite{mozafari_2018}. Training the weights of a convolution,
    
    %% \section{Reservoir Computing}
    %% A reservoir is a type of recurrent neural network architecture. There are
    %% many variants, but they all share a common theme. The reservoir is made of
    %% multiple non-linear units (neurons). These units are randomly connected with
    %% some probability. Inputs are fed into the reservoir, and a layer of neurons
    %% receiving output from the reservoir act as readouts. The shape and level of
    %% connectivity into and out of the reservoir, as well as the connectivity
    %% within the reservoir are tune-able parameters. There are to major benefit
    %% that have driven research into reservoir computing. One, is that SOTA
    %% results can be achieved with only a single trainable layer, the readout
    %% layer. The reservoir is able to extract features from input data in an
    %% unsupervised way, allowing a single layer to perform classification. In
    %% fact, multiple tasks can be performed on the same input, using the same
    %% reservoir with different readout layers. Second, is that reservoirs have
    %% memory, and can seamlessly handle time-series data \parencite{schrauwen_2007}.
    
    %% \subsection{Liquid State Machines}
    %% LSMs are a type of reservoir computing architecture, built from spiking
    %% neurons. In general they are formed from random between neurons in some a
    %% pool. Neurons of the LSM take in some input, which is then projected to some
    %% transient internal state. Readout neurons tap into a feature representation
    %% derived by the LSM, and produce a linear readout. Classification performed
    %% on this readout layer generally provides good results, and with only one
    %% trainable layer.  LSMs naturally have a fading memory, and lend themselves
    %% to time-series data. \parencite{wang_2016}
    
    %%%%%%%%%%%% SNN Learning Approaches %%%%%%%%%%%%
    \section{SNN Learning Approaches}
    In biology, the changing of synaptic efficacy is referred to as synaptic
    plasticity, and is considered to be one method that facilitates learning and
    memory. Changes in plasticity can be quick, such as with pulse-paired
    facilitation (\gls{stdp}-like learning) or more gradual, such as with long-term
    potentiation \parencite{ponulak_2011}.


    %%%%%%%%%%%% \gls{stdp} %%%%%%%%%%%%
    \section{Spike-Timing Dependent Plasticity}
    One of the most widely used learning approaches employed in \glspl{snn} is
    \gls{stdp}, which is an unsupervised approach and variation of Hebbian
    learning. Classic Hebbian learning can be summed up by ``if neurons fire
    together, they wire together'' meaning coincident firing is rewarded by
    increased synaptic strength. Rate-based Hebbian learning strengthens a
    synapse in response to correlated firing, as shown by Figure
    \ref{fig:heb_rate_stdp}. \Gls{stdp} (also depicted in that figure) is a
    spike-timing based variation, and looks for causation. This means a
    connection is strengthened if a pre-synaptic, input spike is followed by
    post-synaptic spike (downstream neuron fires), and weakened if a
    pre-synaptic spike follows post-synaptic spike.

    \asvgf{figures/heb_diagrams.svg}{Diagram showing \gls{stdp} and rate-based
      hebbian learning $\Delta w$ response}{fig:heb_rate_stdp}{0.8}

    \eq{eq:classic_stdp} describes classic \gls{stdp}
    \parencite{tavanaei_2019}. $\tau$ in this case is a decay constant, which
    sets up en effective time window for the learning rule. If spikes occur
    further apart than this window, they have little or no effect on synaptic
    weight.

    \begin{align}
        \Delta W =
        \begin{cases}
          Ae^{-\frac{|t_{pre}-t_{post}|}{\tau}} & t_{pre} - t_{post} \leq 0, A > 0
          \\ Be^{\frac{|t_pre-t_post|}{\tau}} & t_{pre} - t_{post} > 0, B < 0
       \end{cases} \label{eq:classic_stdp}
    \end{align}

    Swapping the conditions in \eq{eq:classic_stdp} leads to
    \gls{astdp} where correlated spikes result in reduced weight. This
    is useful in some learning rules extending \gls{stdp}.

    \subsection{Properties of Classic STDP}
    Classic unsupervised \gls{stdp} has seen some success in pattern recognition
    tasks, but does fall short in certain areas. In general \gls{stdp} converges
    to recognize repeating spatio-temporal patterns, and complex patterns can be
    recognized by a single neuron \parencite{tavanaei_2019}. \Gls{stdp} does
    have trouble converging in order to differentiate similar (but distinct)
    inputs \parencite{vigneron_2020}. \Gls{stdp} also tends to drive weights to
    favor early spikes, using those early spikes as the main factor in pattern
    recognition. There has been extensive statistical analysis of \gls{stdp},
    both due to it's nature, and since it is known the brain performs Bayesian
    analysis of sensory stimuli \parencite{tavanaei_2019}. As part of this
    analysis it was shown that \gls{stdp} coupled with a WTA configuration of
    neurons, was able to implement the Expectation-Maximization (EM) algorithm
    \parencite{nessler_2009}. In addition, it has been shown that classic
    \gls{stdp} will generally drive weights to either maximally inhibitory, or
    maximally excitatory \parencite{legenstein_2005}.

    %% \todo{Expand stat analysis}.

    %% \todo{cite https://arxiv.org/pdf/1611.03000.pdf}.

    \Gls{mlp} networks satisfy the universal approximation theorem, meaning they
    can approximate any function between their inputs and outputs, arbitrarily
    well given the right topology. \Glspl{snn} share this property, through
    their ability to approximate perceptron neurons using rate-based coding. It
    isn't clear yet whether \glspl{snn} have this property outside of this
    approximation, or using coding schemes other then rate-based coding
    \parencite{legenstein_2005}. When considering \gls{stdp}, this raises a similar
    question: what subset of all the possible weight values can be learned using
    \gls{stdp}, or \gls{stdp}-like rules? For a mapping to be learned, it must be
    stable. For \gls{stdp}, weights generally only become stable when their
    values reach an absolute maximum, or absolute minimum. The question then
    becomes, can \gls{stdp} learn all of the possible permutations of max or
    min, across the weight values. Researches found that on average, a
    supervised variant of \gls{stdp} (with teaching signal to force output
    firing) can converge to a desired weight distribution, if this distribution
    is bi-model \parencite{legenstein_2005}.

    \subsection{Weight Initialization}
    
    With \gls{stdp}, weight initialization is an important step, which can
    impact \gls{snn} performance, and ability to learn. It is common practice to
    use a normal distribution to initialize weights. There are some pitfalls to
    this approach. If the mean is too small, some weights may result in
    ``dead'' synapses, which will never result in a neuron firing. Conversely
    choosing a variance that is too large, will result in some
    synapses (at random) overpowering others, and overall poor convergence
    \parencite{vigneron_2020}. Generally, the best approach is to have a mean
    weight value which allows neurons to fire, with the variance being small,
    compared to the mean.

    \subsection{Reward-Modulated STDP}
    
    Recently, there has been some success with a supervised variant on
    \gls{stdp}, known as \gls{rstdp}. Using a deep convolutional approach, a
    \glspl{snn} was employed to solve the \gls{mnist} digit recognition task
    \parencite{mozafari_2018}. Early layers of the network were trained in an
    unsupervised manor, with classic \gls{stdp}. Later layers were updated using
    \gls{rstdp}. That is, if the output was correct, \gls{stdp}
    was applied. If not, \gls{astdp} was applied. This learning approach has
    basis in biology, with the reward modulation mimicking the activity Dopamine
    and Acetylcholine (ACh). As a proof of concept, researchers applied their
    learning method to a shallow network, with a single trainable layer. They
    used rank order coding, and at most one spike per neuron. The output class
    is determined by which neuron in the output layer fires first, after
    presenting the input. This architecture gave passable results, but wasn't
    well suited to multi-layer training.
    
    For their deep architecture, shown in Figure \ref{fig:rstdp_dcnn} better
    results, an accuracy of 97.2\% which is on par with the state-of-the-art was
    achieved \parencite{mozafari_2018}.
    
    \afig{figures/rstdp_dcnn_arch.png}{Deep convolutional \gls{snn}
      architecture}{fig:rstdp_dcnn}

    \Gls{rstdp} improves classification on similar samples, and trains the network to
    recognize discriminating features, instead of repeating ones.

    \subsection{ReSuMe}
    The ReSuMe learning rule is another variant of \gls{stdp}, which is similar
    to the \gls{rstdp} update rule, with some key differences
    \parencite{mozafari_2018}. With ReSuMe, a single neuron and set of synapses are
    initialized with random weight values. There is a 2nd similar neuron
    which implements the desired behavior. Inputs are provided to both neurons,
    and at each time step weights are updated according to the difference
    between the teaching neuron, and neuron in training. The weight update can
    be described by:

    \begin{align}
      \Delta w = \Delta W_{STDP}(S_{in}, S_{d}) + \Delta W_{aSTDP}(S_{in}, S_{o}) \label{eq:resume_stdp}
    \end{align}

    \noindent where $\Delta W_{STDP}$ and $\Delta W_{aSTDP}$ are the weight
    changes associated with classic \gls{stdp} behavior, and anti- \gls{stdp}
    behavior. In each term, the input spike is considered, but first with
    $S_{d}$ which is the desired output coming from the ideal neuron. Second,
    with the actual output \parencite{mozafari_2018}. Using this learning rules,
    it is possible to converge, from an untrained neuron, to one of similar
    configuration with a desired behavior \parencite{ponulak_2010}. The use of
    an additional synaptic input and the specifics of \eq{eq:resume_stdp}
    creates a unique extension to classic \gls{stdp}. Weights can be updated
    without one of the down-stream neurons firing, and activity on a third
    synapse is influencing plasticity. Beyond that, this is yet another learning
    rule, that has shown success when introducing a third factor to \gls{stdp}.

    %%%%%%%%%%%% ReSuMe %%%%%%%%%%%%
%    \subsection{Teaching-Signal Rules}
    %    \todo{populate this section} \parencite{ponulak_2011} - 417


    \subsection{Promising Results With Coordinated STDP}
    Much of the success in training \gls{snn} with \gls{stdp} relates back to
    some level of coordination, or supervision. Figure
    \ref{fig:coordinated_stdp} shows the general form of a coordinated
    \gls{stdp} rule.

    \asvgf{figures/coordinated_stdp.svg}{\gls{lif} neuron with \gls{stdp}
      learning and generic coordination signal}{fig:coordinated_stdp}{0.8}
    
    A number of papers have shown promising results on benchmark recognition
    tasks, including the \gls{mnist} written digit classification
    \parencite{mozafari_2018} and facial recognition
    \parencite{delorme_2001}. These papers, among others use hand-crafted
    \gls{dog} filters in early convolution layers. End-to-end trainable
    \gls{snn} architectures, for image processing tasks appear to be lacking in
    literature.
    
    A mixture of \gls{dog} filters, \gls{stdp}, and \gls{rstdp} can achieve
    \acrshort{sota} results on \gls{mnist}. Though the network wasn't e end-to-end
    trainable, it did have a neuron-based readout, requiring no external
    classifier \parencite{mozafari_2018}.
    
    An unsupervised, coordinated approach involving \gls{roc} showed that
    \gls{stdp} can be used to learn distinct features from \gls{roc} encoded
    images after \gls{dog} filtering. 3x3 \gls{dog} filters were used, followed
    by \gls{roc} encoding, and then 32 maps generated using an 11x11 receptive
    field. Connections from \gls{dog} maps to \gls{lif} neurons are organized in
    such a way to implement convolution, but still be trainable. If weights are
    updated in one region, due to training, the weights are simultaneously
    updated in the other regions. The key to the success of this approach was a
    type of inhibition between feature maps. If a neuron fires within one map,
    the corresponding neuron is inhibited in the others. The goal was to force
    maps to learn unique features, controlling plasticity through gating of
    neuron activity. Results were promising, and activation of \gls{lif} neuron
    maps showed selectivity for image features such as blobs and contours
    \parencite{delorme_2001}.
    
    %%%%%%%%%%%% Back Propagation %%%%%%%%%%%%
    \section{Back Propagation in Spiking Neural Networks}
    
    Back-propagation is difficult to implement in \glspl{snn}, as the
    spike function is not differentiable. There are some shortcuts proposed,
    such as using the membrane voltage derivative instead of spike
    output. However applying backprop may not be the best approach. The brain
    doesn't (as far as is known) have a method of back-propagating errors. It
    has been proposed to adapt the network architecture to allow rewards at the
    output to affect all portions of the network. A hybrid reward-STDP approach
    shows promising results \parencite{tavanaei_2019}.
    
    %%%%%%%%%%%% Deeper Networks %%%%%%%%%%%%
    %% \section{Deeper Networks}
    %% Deep learning is still largely unexplored in \glspl{snn}, as well as multi-layer
    %% learning in the deep architectures that have been explored. Motivating
    %% further exploration is both the success of deep Artifical Neural Networks,
    %% and the observation that the mammalian brain relies on a deep architecture
    %% for visual tasks such as detection and recognition \parencite{tavanaei_2019}.
    
    %%%%%%%%%%%% Astrocytes Intro %%%%%%%%%%%%
    \section{Introduction to Astrocytes}
    Astrocytes are a type of glial cell found in mammalian brains. Their
    structure and function are still the topic of cutting edge research
    today. It is known that they are vital for normal brain functions, including
    cognition and behavior \parencite{mederos_2018}. In the human brain astrocytes
    are known to tightly wrap many synapses, as well as dendrites and cell
    bodies. Since astrocytes engulf the synaptic cleft, they are ideally
    placed to control extracellular neurotransmitter and ion
    concentrations. There is substantial evidence that they regulate
    extracellular \kp, which is required for propagation of action potentials
    through neuron bodies. \Gls{gaba} transporters in astrocyte cell membranes
    serve to clean up neurotransmitters, and can help mitigate excitotoxicity. It
    should be noted that astrocytes do not wrap, or even influence every
    synapse, and that their density varies widely depending on the region of the
    brain. More interesting, their morphology can change throughout life, and in
    response to other bodily functions, such as food intake \parencite{mederos_2018}.

    From a functional point of view, at a high level, astrocytes listen and
    respond to activity at the synaptic clefts of multiple synapses. Activity on
    a given synapse leads to a local response, in the astrocyte process that
    surrounds that synapse. Local responses from many synapses contribute to a
    global cell-body response. This global response may propagate to other
    astrocytes, and the responses from other astrocytes may influence, or even
    trigger a global response of an astrocyte \parencite{min_2012}. At a given
    synapse, astrocytes are sensitive to pre and post synaptic potentials. They
    sense this activity via uptake of \gls{glu} released at the presynaptic
    terminal and \kp ion uptake at the postsynaptic terminal. In the case
    of inhibitory synaptic transmission, astrocytes take up and respond
    \gls{gaba}. Depolarization via \kp at the post-synaptic terminal can be
    sensed as well, through the voltage change, and uptake of \kp by an
    astrocyte. The astrocyte will then respond with one or more
    \gls{gt}. These include \gls{glu}, D-serine, \gls{atp} or
    \gls{tnfa}. These \gls{gt} have different effects depending on where
    and when they are released.

    \begin{itemize}
      \item \Gls{glu}: When released to the pre-synaptic neuron \gls{glu} generally
        results in an increase in Probability of Release (PR). This equates to
        an increase in the weight for a computational model. At the
        post-synaptic terminal, \gls{glu} release results in depolarization, or
        a so called slow inward current (SIC) \parencite{pitta_2016}.
      \item D-Serine: this neurochemical gates the \gls{ndma} receptors, and
        subsequently gates \gls{ltp}/\gls{ltd} \parencite{mederos_2018}.
      \item \Gls{atp} has a depressive effect, opposite to \gls{glu}, and can
        propagate beyond the synaptic cleft \parencite{mederos_2018}.
      \item \Gls{tnfa} results in an increase in the number of post-synaptic
        surface \gls{glu} receptors, and a decrease in \gls{gaba} receptors
        \parencite{chung_2015}.
    \end{itemize}

    \Gls{gt} release from an astrocyte is dependent on the integration of
    synaptic activity over 100s of milliseconds (for a local response)
    \parencite{pitta_2016} to 10s of seconds for a whole cell response
    \parencite{mederos_2018}. This integration is mediated by a variety of input
    pathways, some of which are more thoroughly understood than others. These
    input pathways generally converge to provide an increase to intracellular
    \ca concentration, within an astrocyte; though there is some debate surround
    \ca signaling within astrocytes \parencite{mederos_2018}. Initial results of
    in-vitro experiments showed a slow \ca response in astrocytes following
    intense neuronal activity. This showed that while astrocytes were active,
    they could not exert rapid or granular control over information flow at the
    synapse. More recent research with more advanced methods has shown that
    there are synapse local \ca responses that are much quicker and respond to
    lower levels of activity \parencite{araque_2014}. The connection between local
    and cell level response is mediated by \gls{cicr}. As local activity
    increases beyond a certain threshold, the \ca concentration causes
    exponential \ca release from the \gls{er}. This \gls{cicr} propagates like a
    wave, and can reach the main cell body, other astrocyte processes, and even
    other astrocytes \parencite{manninen_2018}.

    %% TODO: Add in the \gls{cicr} \gls{er} picture, it shows how \ca waves can
    %% propagate to the cell soma
    %% TODO: More detail here

    \section{High Level Roles for Astrocytes In Biology}

    There are a variety of theories of the functional role of astrocytes.
    Some of these are speculation based on general themes of astrocyte behavior,
    and others are specific and highly grounded in experimental data.

    %% Astrocytes are Master Integrators
    Astrocytes have been shown to act as integrators of synaptic activity. Early
    studies discovered that astrocytes become active in response to intense
    neuronal firing. Subsequent research has shown that this response, represented
    by \ca concentration, was highly complex in time
    \parencite{araque_2014}. This complex global response which happens on a
    seconds timescale is preceded by \ca activity in astrocyte processes local
    to synapses, and at regional levels between local and global. It is
    hypothesized that these local \ca increases (sometimes called ``puffs'' or
    ``sparks'') integrate to drive additional responses. In this way astrocytes
    can locally integrate neuronal activity temporally, on a 100s of
    milliseconds time-scale, then temporally and spatially integrate on slower
    time-scales. There is evidence of this multi-level arrangement. When looking
    at astrocyte \glspl{mglur} along a common astrocye process, there are
    clusters of receptors which form a local region. Each of these regions is
    associated with synaptic terminals, and may integrate their activity at
    different spatial and temporal time-scales as their neighbors
    \parencite{pitta_2012}. Furthermore, experiments with computational models
    have shown that varying astrocyte modalities (as is the case in the brain) led
    to different patterns of signal transmission. This could indicate that
    astrocytes in different areas of the brain perform different functions
    \parencite{pitta_2012}.

    %% Astrocytes facilitate long-range spatial influence
    A single astrocyte, through its many end-foot processes can influence many
    synapses simultaneously. In the brain, astrocytes are physically distributed
    via a mechanism called contact spacing, where their end-foot processes
    connect at the periphery of an astrocyte's domain
    \parencite{pitta_2012}. This spacing is not always uniform, some
    micro-domains are formed favoring a neuron signal pathway, with adjacent
    astrocyte connections observed to be absent. One example is in the ferret
    visual cortex, where astrocytes (like neurons) form receptive fields on the
    visual input \parencite{pitta_2012}. This finding lines up well with results
    in other works, where astrocytes were used as working-memory units, forming
    receptive fields for visual information \parencite{gordleeva_2021}.

    Signals propagate in these astrocyte networks in a few different
    ways. One pathway, involves \ipt diffusion through astrocyte gap
    junctions. Once across, sufficiently high levels of \ipt cause
    \gls{cicr}, and in turn astrocyte waves. \ca may also diffuse across gap
    junctions in the same way. This propagation is dubbed \ca waves, due to
    the wave-like nature of the propagation. In addition to the gap-junction
    mediated effects, \gls{atp} released from an astrocyte may diffuse
    extra-cellularly, and influence other astrocytes \parencite{amiri_2013}.
    
    %% Astrocytes Modulate STP/STD
    The pre-synaptic \gls{glu} mediated astrocyte stimulation and response loop,
    is hypothesized as a mechanism for modulation of short term plasticity
    \parencite{pitta_2012}. This loop is characterized by positive feedback,
    where \gls{glu} release from the pre-synaptic neuron stimulates the
    astrocyte, and leads to additional \gls{glu} release. If the \acrshort{pr}
    at that synapse is low, then there should be sufficient neural resources to
    support transmission, and the astrocyte is able to gain-up the response. If
    \acrshort{pr} is high, then the astrocyte response leads to short-term
    depression, as neural resources are exhausted quickly.

    %% Astrocyte Modulation of \gls{ltd}/\gls{ltp}
    Astrocytes have been shown to play a key role in the bio-chemical pathways
    that lead to \gls{stdp} at neuronal synapses, and can gate \gls{ltd} and
    \gls{ltp} via D-Serine release \parencite{manninen_2019}.

    Both \gls{ltd} and \gls{ltp} are controlled by the \gls{ndma}
    receptor. Through gating factors this receptor is able to respond to the
    co-incidence of pre and post-synaptic action potentials. Figure
    \ref{fig:astro_plastic} shows astrocyte involvement in this process. In the
    case of \gls{ltd}, when a post-synaptic depolarization is followed closely
    by pre-synaptic depolarization a signaling pathway proceeds, leading to
    \gls{glu} release by the astroctye, which in turn triggers \gls{ndma}
    receptors in the pre-synaptic neuron. The influx of \ca leads to a decrease
    in pre-synaptic neurotransmitter release probability. Beyond passively
    participating in the normal \gls{stdp} process, astrocytes may gate it's
    activity, or reverse the polarity, implementing \gls{astdp}
    \parencite{min_2012}. This is achieved through \gls{gt} release, such as
    D-serine, which provides a necessary gating factor for \gls{ndma} receptors
    to open. \Gls{glu} released by astrocytes in response to pre and
    post-synaptic activity may stretch or shift the nominal \gls{stdp} curve
    in both experimental results, and computational models \parencite{pitta_2016}.

    Astrocytes can modulate the concentration of their surface glu transporters,
    this in turn modulates the level of glu spill-over, beyond the synapse. This
    increases the excitability (depolarizing neurons partially towards firing)
    at the post-synaptic terminal for a local region of synapses. Astrocytes may
    also respond to \gls{glu} activity (as sensed by transporter uptake) by
    releasing more \gls{glu}, or by releasing \gls{atp}, based on signaling
    frequency. Within the Hippocampus, it has been observed that \gls{gaba}
    release from a pre-synaptic neuron can be taken up by \gls{gaba}
    transporters on an astrocyte's surface. This leads to an increase in \na /
    \ca within the astrocyte, and the eventual release of \gls{atp}. \Gls{atp}
    acts as an inhibitor, down-regulating excitatory transmission
    \parencite{mederos_2018}.
        
    \afig{figures/astrocyte_ltd_ltp.png}{Astrocyte-mediated plasticity.}{fig:astro_plastic}

    \section{Biochemistry Inspires Computational Models of Various Pathways}
    Over the last 30 years there have been experiments and studies surrounding the
    role of astrocytes in the mammalian brain. This research has led to the
    identification of a variety of signaling pathways, along with some
    quantitative data. These new insights in the neuroscience field, have led
    to a wave of computational models, which attempt to either mimic
    neuron-astrocyte interactions, or extract some computational benefit by a
    simplified, but still very bio-plausible model. The first clue to astrocyte
    involvement in computation was the observation of \gls{glu} released from
    the pre-synaptic neuron, and subsequent increase in \ca concentration
    within astrocyte cell bodies and foot processes \parencite{manninen_2018}. Around
    2010, new experimental data emerged from in-vivo studies, which furthered
    the functional understanding of astrocyte signaling
    \parencite{manninen_2018}. Some pathways are responsible for modulating
    plasticity while other lead to transient changes in neuron dynamics. A key
    step in developing a computational model, that captures the key features
    involved in information processing, is understanding the biological
    pathways.

    \asvgf{figures/astro_neuron_pathways.svg}{Common astrocyte-neuron pathways,
      and common pathways for internal \ca signaling}{fig:astro_pathways}{0.6}

    Figure \ref{fig:astro_pathways} shows the possible signaling pathways at the
    tripartite synapse. Each pathway has some underlying research into the
    behavior at that pathway, either in vitro or in vivo. These experimental data
    have led to a variety of computational models, which share a common theme but
    differ in complexity, level of bio-realism, and computational efficiency.

    \subsection{Neuron-Astrocyte Pathways - Presynaptic to Astrocyte}

    One of the first experimental observations of astrocytes was a transient
    increase in \ca in response to high levels of neuron activity. One of the
    main pathways mediating this response is the Pre-synatic \gls{glu}
    pathway. Excitatory cortical neurons release \gls{glu} in response to input
    stimulus. \Gls{glu} binds to the G-coupled \glspl{mglur} on the surface
    of an astrocyte. This sets in motion a cascade involving the \ipt second
    messenger, which ultimately leads to \ca release in the astrocyte cytosol
    \parencite{pitta_2012}. \ipt induced \ca release from the \gls{er} leads to a rapid
    breakdown of \ipt, creating a kind of local \ca spike. This \ca
    concentration can integrate within the cell cytosol, but does degrade due to
    the activity of pumps at \gls{er} surface \parencite{pitta_2012}.

    To model this computationally, a three variable approach, which extended the
    Liâ€“Rinzel model \parencite{li_1994} has been used \parencite{pitta_2009}. In
    general \ca concentration (when considering the \ipt mediated pre-synaptic
    pathway only) is dependant on two internal activities, and one
    external. Internally, there is a $J_{leak}$ factor, describing a
    differential based leak from the \gls{er} into the astrocyte cytosol. To
    override this leak, and maintain the differential, \gls{serca} pumps move
    \ca into the \gls{er}. This is noted by $J_{pump}$. $J_{chan}$ accounts for
    the flux of \ca from the \gls{er} into the cytoplasm due to \ipt levels, or
    \gls{cicr}, depending on the \ca concentration \parencite{pitta_2009}. In
    this case \ipt levels are a function of \gls{glu} release from the
    pre-synaptic neuron. This model was extended to include the \gls{gt} release
    from the astrocyte in response to \ca transients. In their work a spike of
    \gls{gt} is released (similar to a neuron) when \ca concentration reaches
    some threshold. It should be noted, that the \ipt/\ca spiking response (\ipt
    $\rightarrow$ \ca release $\rightarrow$ \ipt cleanup and \ca cleanup) is
    independent of the \gls{gt} release, meaning there can be sub-threshold \ca
    spikes) \parencite{pitta_2016}.

    The above outlines the common theme for presynaptic astrocyte
    modulation. The general form of the pathways outlined above was shared by many
    notable works, such as those by Postnov and Wade \parencite{postnov_2009,
      wade_2011}.

    \subsection{Neuron-Astrocyte Pathways - Postsynaptic to Astrocyte}
    Another common pathway that is explored both in neuroscience and
    computationally is the ``fast'' post-synaptic pathway
    \parencite{bassam_2015}. In this pathway, the post-synaptic neuron fires,
    and subsequently releases \kp as part of the depolarization process, at the
    postsynaptic terminal. This \kp spillover is quickly shuttled into the
    astrocyte, and causes depolarization across the astrocyte's
    membrane. Voltage-gated channels on the \gls{er} then lead to \ca release
    into the cytoplasm. It is considered fast pathway, because the effect of \kp
    is direct, vs. the pre-synaptic pathway involving a second messenger
    \parencite{bassam_2015}.

    The post-synaptic pathway is modeled differently across reviewed
    literature. One configuration used the Gerstner spiking
    response model \parencite{gerstner_2001}, and explored the \ca response to a
    post-synaptic neuron using \eq{eq:kp_path_srm}. This
    characterized the response to a single post-synaptic spike as a portion of
    an exponential function, similar to the \gls{psp} generated by a
    pre-synaptic spike \parencite{bassam_2015}. Other works incorporated the \ca
    response into a FitzHughâ€“Nagumo neuron model, using \eq{eq:kp_path_fn},
    where current was injected into the synapse, subject to \eq{eq:fitz_nn}
    \parencite{postnov_2007}.

    \begin{align}
        PS_{mod} =
        \begin{cases}
          \alpha W + We^{\frac{-t - t^k_{i}}{\tau_s}} & t^k_{i} < t < t^k_{i}
          + d \\
          \alpha W + We^{-\alpha (-t - t^k_{i})} & otherwise
       \end{cases} \label{eq:kp_path_srm}      
    \end{align}

    Where $W$ is the sum of all synaptic weights $t^k_i$ is the time of the last
    spike on the $i$-th synapse. $\alpha$ is a constant, and $t$ is time.

    \begin{align}
      \tau_c \frac{dc}{dt} \alpha (r + \alpha w2 + \beta S_m) \label{eq:kp_path_fn}
    \end{align}

    Where $\tau_c$ is a time-constant, $r$ controls the initial state, and
    $\alpha W2$ represents the effect of post-synaptic depolarization on
    \ca. $W2$ is analogous to $W1$ from \eq{eq:fn_neuron}, which
    defines the FitzHugh - Nagumo model.

    Though the exact dynamics of the response are dependent on the specific
    neuron model in use, the general form of the response is the same as
    outlined above.
    
    \subsection{Calcium, and Other Messenger Dynamics}
    \ca, \ipt and \kp are the main substances involved in signaling pathways
    within the astrocyte. Looking closely at both the neuroscience literature,
    and modeling efforts, there are spikes and thresholds within these internal
    astrocyte concentrations. Neuroscience experiments involving monitoring of
    astrocyes revealed oscillations, which are more intense and propagate
    further depending on Activity. These oscillations are the consequence of
    non-linear negative feedback, leading to fast cleanup, and spike-like
    behavior \parencite{postnov_2009}. The second messenger \ipt has a similar
    behavior, with a threshold and $tanh$ (with decay factor) resulting in
    spike-like behavior \parencite{postnov_2009}. This response was not limited
    to internal signals, with some models considering spikes of \gls{gt}
    \parencite{wade_2011}.

    The amplitude modulated and frequency modulated variants on the $J_{pump} -
    J_{chan} - J_{leak}$ model exhibit spiking and non-spiking \ca dynamics depending on some choice
    parameters \parencite{pitta_2009,wade_2011}. It is also well established
    that the \ipt pathway operates on a slower time-scale, then more direct
    pathways such as \gls{atp} or \kp \parencite{postnov_2009, bassam_2015}.

    \section{Evolution of Astrocyte Models}
    A very insightful review of astrocyte models from 1995 until about 2017 was
    used to guide the investigation into existing computational astrocyte models
    \parencite{manninen_2018}. When the existing body of work was considered it
    was possible to lean on functional similarities between Astrocyte models and
    identify four distinct core neuron-astrocyte models, which emerged over the
    time-period considered \parencite{manninen_2018}.

    \subsection{Foundational Astrocyte Models}
    Grouping models by their features, it was shown in \parencite{manninen_2018}
    that only a few fundamental computational astrocyte models make up the core
    of many published works. These groups were formed by Lin and Rinzel-like
    models, DeYoung and Keizer, and Hofer. Lin-Rizel and DeYoung models share a
    common high-level definition, with models consisting of three terms defining
    \ca dynamics. $J_{pump}$ to denote the flux of \ca to maintain a gradient
    between astrocyte cytoplasm, and \gls{er}, $J_{leak}$ to denote a constant
    leak factor from \gls{er} to cytoplasm, and $J_{chan}$ to denote \ca influx
    due to outside influence. Treating these as simultaneous differential
    equations, overall \ca response $\frac{dc}{dt}$ can be defined, see
    \eq{eq:lr_dk_astro}.

    \begin{align}
      \frac{d[\cam]}{dt} = J_{chan} + J_{leak} -
      J_{pump} \label{eq:lr_dk_astro} \\
    \end{align}

    The Hofer model on the other hand, defines an astrocyte model in a more bio-realisitc
    way, modeling internal chemical signaling using second-order differential
    equations, and including messengers beyond just \ipt. There is also an
    emphasis placed on astrocyte to astrocyte communication through gap
    junctions, which is unique to this model. Overall Hofer-like models were the
    least prevalent in existing literature \parencite{manninen_2018}.

    
    \section{Astrocyte Mediated Effects in the Literature}
    \subsection{Glio-Transmitter Release From Astrocyte}
    To signal back to neurons, astrocytes release various \gls{gt},
    which were outlined above. In general, this release is dependent on the
    concentration of \ca local to the synapse. Input pathways converge with
    \ca concentration, and then divergent effects are observed via the release
    of multiple \gls{gt}.

    \Gls{atp} release from astrocytes can travel within interstitial spaces and
    effect synaptic transmission at physically local synapses. This phenomenon
    of hetero-synaptic suppression, which is mediated by \gls{glu} and \gls{atp}
    release is explored in \parencite{postnov_2009}. Experiments depend on an
    existing computational model, with some modifications to support \gls{atp}
    release. A synapse local to an astrocyte is potentiated in the short term
    via \gls{glu} release. Over a longer time-scale, \gls{atp} is released from
    the astrocyte and diffuses to a neighboring synapse, decreasing synaptic
    PR. This signaling can bypass both synaptic connections, and astrocyte gap
    junctions, effecting physically local synapses and neurons.

    \subsection{Facilitation of \Gls{ltp}/\Gls{ltd}}
    Astrocytes are believed to facilitate, or even implement \gls{ltp}/\gls{ltd} in
    biological synapses. Retrograde signals responsible for synaptic PR changes
    are picked up by astrocytes, and result in the release of \gls{glu} and D-Serine,
    which act on pre-synaptic NMDA receptors. Activation of these receptors
    leads to long-lasting changes in synaptic strength \parencite{min_2012}.

    \section{Propagation of Astrocyte Signals Internally}
    Early research into astrocyte behavior showed cell-wide \ca transients in
    response to intense activity, which would in some cases propagate to
    neighboring cells. Around 2010, pharmacological tools became precise enough
    to discover smaller variations in \ca at astrocyte processes, in response to
    lower levels of neuronal activity \parencite{manninen_2018}. These smaller
    variations are thought to be integrated, and lead to a larger cell-level \ca
    response, though there is not direct evidence of this
    \parencite{araque_2014}. In addition, the mechanism thought to trigger the
    switch from local to global responses is \ca-Induced \ca release
    (\gls{cicr}) which is the mechanism responsible for spike-like behavior
    within astrocyes.

    Astrocytes also form their own networks, separate from the connections of
    neurons. Gap junctions between cells can pass various molecules including
    ions and secondary messengers. Gap junctions are not evenly distributed, or
    random, meaning they form meaningful connections between specific
    astrocytes. These networks appear to form non-overlapping territories, where
    groups are interconnected, but distinct from other
    groups \parencite{mederos_2018}. Astrocytes don't have long-reaching
    projections like the axons of neurons, limiting \ca propagation to a few
    $\mu m$ \parencite{hofer_2002}. The shapes of these networks are varied,
    and in the visual cortex consist of between 2 and 10 astrocytes. In
    addition, Astrocyes are generally not found un-coupled in the brain, further
    supporting the significance of astrocyte networks \parencite{postnov_2009}. The
    specific molecules that generally diffuse across gap junctions are \ipt and
    \ca, with only \ipt being modeled in some cases \parencite{pitta_2012}. In any
    case the effect is generally described as excitable, regardless of the
    molecule that diffuses \parencite{gordleeva_2021, pitta_2012, postnov_2009}.

    The functional role, and information processing implications of astrocyte
    networks isn't yet well understood. There are a few consequences of
    astrocyte networks that have been observed. First, these networks provide
    additional pathways for \ca waves to propagate, and in some cases form
    cycles, where astrocyte waves return to their place of origin. In addition,
    under intense neural activity, ANs support far reaching synaptic modulation,
    through wide reaching \ca wave propagation \parencite{postnov_2009}. There
    have been attempts at modeling astrocyte network, with variying degrees of
    complexity. A random procedural algorithm for generating a 2D bio-plausible
    astrocyte network was able to show a high level of bio-realism, and mimic
    the behavior of \ca waves \parencite{postnov_2009}. The algorithm is as
    follows.

    \begin{enumerate}
        \item From the center of each astrocyte, produce a random number between
          m and n of branches pointing outward in uniformly random directions.
        \item If any branch either intersects another of the same astrocyte, or
          is closer to another astrocyte than its ancestor (could be its
          astrocyte, or another branch), then the branch is rejected, and
          another random branch is generated in an attempt to take its place.
        \item For any surviving branches generate a random number of next-level
          branches between 1 and k.
        \item repeat 2, 3 until a maximum branching level $N_{max}$ is reached.
    \end{enumerate}

    Using the novel astrocyte connection algorithm developed, some important
    observations of astrocyte signaling were made in simulation
    \parencite{postnov_2009}. First, low-levels of synaptic activity elicited a
    strictly local response. Increasing the level of stimulation results in a
    \ca wave, which spans many astrocytes, and propagates out from its
    source. With sufficiently active neurons, the resulting \ca wave is shown to
    propagate out from an astrocyte, and then eventually return to its
    source. The main goal here was to match experimental results outlined
    neuroscience research, and there was minimal discussion as to a functional
    role.

    Using similar properties, astrocytes were shown to be capable of
    implementing short-term memory, both by maintaining an internal state over
    time, and through influence of neighboring astrocyte through gap
    junctions. Once memory is stored, it was retrieved through presentation of
    a cue input \parencite{gordleeva_2021}.

    %% \section{Related Work - Organized By Paper and Author}
    
    %% There have been a variety of works attempting to bring an Astrocyte model
    %% into ANNs. In general these models were either not biologically inspired
    %% enough, or too complex in their modeling of Astrocyte activity for real-time
    %% and scalable use \parencite{bassam_2015}. These models generally consisted of,
    %% in the simple case, a supervising element per neuron. If this neuron emits
    %% sufficiently many spikes in some time-frame, the Astrocyte will increase the
    %% downstream weighs for that neuron. Alternatively if a neuron is inactive,
    %% measured by fewer than some number of spikes within a time-frame, downstream
    %% weights will be decreased \parencite{mesejo_2015}. Models like this one
    %% however, have no concept of \ca signaling, and can't easily support
    %% Astrocyte networks.
    
    %% \parencite{bassam_2015} proposes a Spiking Response Model which uses \ca
    %% concentration as an internal state variable, and incorporate multiple
    %% pathways to affect a change in \ca. Figure \ref{fig:srm0} gives a
    %% high-level overview of this model. Spikes from some number of neurons enter
    %% the synapse. A key difference in this work is the emphasis placed on the
    %% difference in time-scales between the Pre-synaptic slower \gls{glu} -> \ipt -> \ca
    %% pathway, vs the postsynaptic faster \kp -> \ca pathway.
    
    %% \begin{align}
    %%     V_j(t) = \eta(t - t_j) + \Sigma^{N_{i+1}}_{i=1}W_{ij}\Sigma^{K_i}_{k=1}
    %%     \epsilon(t - t_i^k - d_{ij})
    %% \end{align}
    
    %% Where $V_j(t)$ represents the membrane potential of a neuron, with $K_i$
    %% spikes coming from each of $N_{i+1}$ neurons. The voltage change associated
    %% with each spike is the product of the weight $W_{ij}$ and a response kernel
    %% $\epsilon$.
    
    %% \afigs{figures/snn_model.png}{Spiking Response Model}{fig:srm0}{0.6}
    
    %% The spike response kernel $\epsilon$ is defined by \eq{eq:srm_eps}
    %% \begin{align}
    %%     \epsilon (s) = \frac{s}{\tau s} e^{\frac{s}{\tau s}}
    %%     H(s) \label{eq:srm_eps} \\ s = (t - t_i^k - d_{ij}) \\ H(s) =
    %%     \begin{cases} 
    %%       1 & s >= 0 \\ 0 & s < 0
    %%    \end{cases}
    %% \end{align}
    
    %% Where $H(s)$ can be recognized as the Heavy-side Step Function. After a
    %% spike, the neuron enters a refractory period.
        
    %% \begin{align}
    %%     ca^{2+} = r + S_{mod} + PS_{mod} \label{eq:srm_astro_ca} r = 0.31
    %% \end{align}    
    
    %% %%%%%%%%%%%%\glspl{snn}on an FPGA %%%%%%%%%%%%
    %% \section{SNNs on an FPGA}
    %% \parencite{cassidy_2017} have shown the \gls{lif} based\glspl{snn}can be implemented on FPGA
    %% using adders to numerically integrate discrete spikes. \gls{stdp} weight updating
    %% was also included in the implementation. Figure \ref{fig:fpga_lif} shows the
    %% hardware implementation of an \gls{lif} neuron. The normal continues \gls{lif} equations
    %% collapse to an adder with a negative bias.
    
    %% \afigw{figures/fpga_spike.png}{FPGA Implementation of \gls{lif}
    %%   Neuron}{fig:fpga_lif}{2}
    
    %% Traditional ML tasks weren't performed on the hardware spiking network, but
    %% a few experiments were performed, which may provide insight. First network
    %% weights were optimized to provide a maximally informative Spacio-temporal
    %% receptive field (STRF) for English speech. This is a key step in voice
    %% recognition. Second the researchers exploited a well-known property of STDP
    %% to validate network and training behavior. Each input was driven by a
    %% Poisson spike train, each with the same $\lambda$. The expected result
    %% (after some time) is that the network will reach a steady state, with an
    %% equal number of exitatory and inhibitory weights. This result was observed,
    %% validating the FPGA implementation aligns with the expected result.
    
    \section{Theories on High-level Functions of Astrocytes Biology}
    
    It is important to speculate on some higher level functional roles of
    astrocytes, even when the availability of concrete data isn't sufficent to
    test these hypotheses. In general astrocytes can modulate synapse
    excitability through a variety of mechanisms, and modulate synaptic
    plasticity through the release of \gls{glu} and D-serine. There is evidence
    that astrocytes modulate \gls{ltp}/\gls{ltd} based on external inputs and internal
    state, this could provide a mechanism for \gls{rstdp}
    \parencite{min_2012}. In addition It has been shown that smaller learning rates
    coupled with \gls{stdp} lead to better memory, when presented with test and
    challenge inputs \parencite{van-rossum_2012}. Astrocytes modulate learning at a
    regional level, providing an opportunity for one portion of the network to
    learn, while another retains what it learned previously. In addition to
    regional gating (or more fine-grained modulation) of learning, astrocytes,
    through local differences in response (within the same cell) can bias a
    network of spiking neurons to learn a particular function, or class of
    functions.

    In addition to plasticity, Astroctes can modulate the synaptic tone, or
    basal level of excitation. It has been suggested that this effect, coupled
    with inter-astrocyte communication can "activate" a region of a network,
    possibly choosing it as most important for the given input. In essence,
    astrocytes would implement a context switch \parencite{min_2012,
      gordleeva_2021}. Another result of this regional excitation is
    synchronization. As astrocytes lower the threshold for post-synaptic firing,
    pre-synaptic potentials that are disparate in time, or strength can result
    in synchronized post-synaptic firing. This effect could be important in a
    variety of situations, but one of particular note is STDP. Since \gls{stdp}
    is very sensitive to relative spike timing, having synchronized inputs (and
    not penalizing an input for being ever so slightly late) would result in
    better stability.

    Given astrocyte's level of connectedness with neurons, and the formation of
    astrocyte networks, there is a distinct opportunity for information transfer
    across long distances without the need for supporting neural
    connections. It is thought that these pathways underlie some of the
    coordinated activity between neural circuits from different brain
    regions. Some researchers even claim astrocytes for the basis of human
    consciousness.
